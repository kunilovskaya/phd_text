\chapter{\label{cha:indicators}Translationese Indicators}
This thesis explores the relation between theoretically-motivated translationese features and available quality judgments on student translations. It also investigates translationese-related distinctions between professional varieties of translations. 
The goal of this research makes feature engineering and selection a cornerstone of the project and necessitates an overview of translationese-related studies with a focus on recent developments in feature-based approaches and related research methods.

The term \textit{translationese} was introduced by~\citet{Gellerstam1986}. A few year later, after \citet{Baker1993} convincingly called for translation scholars to shift attention from \gls{ST}-\gls{TT} relation, which traditionally cast translations as unworthy `second-hand' texts, to objective properties of translations distinguishing them from non-translations in the TL, the phenomenon of translationese has become one of the main objects of study in empirical (or corpus-based) translation studies\wlvfootnote{\citet{Baker1993} did not use the term `transaltionese'. She put forward a few hypothesis about linguistic properties of translations and refered to them as `translation universals'. See discussion below.}. 
Today, the study of translationese is a well-established research field, which relies on computational methods. It gave rise to a number of interrelated \gls{NLP} tasks such as translation detection, translation direction and SL identification. Machine learning methods are used to investigate variation in translation along a number of dimensions such as register, professional competence, source language, translation method (automatic vs. human), etc.

This chapter opens with the key theoretical concepts in the field and focuses on the more recent developments regarding feature engineering and text representation (Section~\ref{sec:linguistic}). Section~\ref{sec:feats4qua} presents theoretical underpinnings and existing evidence that substantiates our main hypothesis that the amount of translationese is correlated with translation quality. Section~\ref{sec:myfeats} has a detailed description of translationese feature sets proposed in this research.  

\section{\label{sec:linguistic}Linguistic Specificity of Translations}
\subsection{\label{ssec:keyterms}Key Concepts}
% translationese (its connotations and uses), universals aka tendencies, translationese classification, 
This thesis builds on the observation that translated texts have regularities in their linguistic form making them distinct from texts generated outside the communicative situation of translation. These unique characteristics of translations are attributed to the specificity of the underlying communicative process. They are the `fingerprints' of the translation process on the product~\cite{Gellerstam1986}.

\label{pg:three_strands_in_translationese_studies}
The observed peculiarities of translated language gave rise to a number of explanatory theories that focused various aspects of this phenomenon and suggested their own terms to refer to it. The major schools of thought can be distinguished based on whether the specificity of translations is attributed to (i) the factors, independent of the language pair involved, (ii) to the confrontation of (specific) source and target languages or (iii) mostly to the SL interference with some normalisation pull from the TL. 

The first strand of research can be best represented by the `translation universals' interpretation, put forward by~\citet{Baker1993}. She argues that translations demonstrate properties that cannot be linked to respective SLs, but reflect the constraints of the translation process per se. According to~\citet[p.243]{Baker1993}, universal features of translation are ``features which typically occur in translated text rather than original utterances and which are not the result of interference from specific linguistic systems''. She discusses four hypotheses that might explain the specific pattering of translations~\cite[pp.176--177, 183--184]{Baker1996}: 
\label{pg:major_trends}
\begin{itemize}\compresslist{}
	\item \textit{simplification} (a subconscious tendency to simplify the language or message or both of the source),
	\item \textit{explicitation} (a tendency to spell things out rather than leave them implicit), 
	\item \textit{normalisation/conservatism} (a tendency to conform to patterns which are typical of the target language) and
	\item \textit{levelling-out} (a tendency to gravitate towards the center of a continuum). 
\end{itemize}
\citet{BlumKulka1986}, who originally introduced the explicitation hypothesis, made a conclusion that cohesive patterns observed in translations were neither SL- nor TL-oriented. 

The second explanation is offered by some scholars who characterise the linguistic makeup of translations as `third language'~\cite{Duff1981}, `third code'~\cite{Frawley1984} or `hybrid text'~\cite{Schaffner2001a,Schaffner2001b}. They tend to explain the distinctive features of translations by the confrontation of specific linguistic systems, which result in mixed `strange' patterns of linguistic expression making translations their own TL subsystem. 

The third approach attributes deviations of the translated language from the expected TL norm primarily to the SL influence. \citet{Gellerstam1986} and~\citet{Santos1995}, who use the term `translationese' to refer to linguistic properties of translations, tend to consider source language influence the major cause of linguistic specificity of translations. For example, \citet{Santos1995} uses the term `translationese' ``to denote the influence of properties of the source language in a translated text'' (p.61). Lexical generalisations made by~\citet{Gellerstam1986} reflect the various degrees of impact of the SL, although he acknowledges that not all translationese is explainable by interference: cultural differences and cross-linguistic divergences between registers are other important sources of translationese. A more radical stance is taken by~\citet{Rabadan2009}, who defines translationese as ``features of translated language that can be attributed to the influence of the source language'' (p.303).

\citet{Toury1995} maintains a descriptive approach to the study of translated texts. He explains the specificity of translations through `laws' (most importantly \textit{the law of interference} and \textit{the law of growing standardisation} also known as normalisation, i.e. a tendency to adopt to linguistic expression in translation to the prototypical features of the target language~\cite{Denturck2009}) and `norms', i.e. socio-cultural constraints regulating the behaviour of professional translators. The latter can be exemplified by `language prestige', an idea that unequal status of languages and cultures could affect tolerance of interference. This effect was demonstrated in a study of Finnish translations from English and Russian. \citet[p.72]{Mauranen2004} concluded that there was a greater tolerance in the Finnish culture for English than for Russian interference. Similarly, more SL-related translationese was observed in German translations than in English translations in a bidirectional study on English and German by~\citet{Evert2017}. 

More recent translationese studies, including~\citet{Koppel2011,Volansky2015,Rabinovich2017,Chowdhury2020,Chowdhury2021,Kunilovskaya2020vars} and~\citet{Evert2017} above, demonstrated that a lot of translationese could actually be explained by SL \textit{interference}, or \textit{shining through}, a term introduced by~\citet{Teich2003}. It is explained as follows: ``diverging frequencies of options existing in both languages are adapted in translated texts to those of the source language''. %\todo[inline]{add page}
Note that some authors use the term \textit{interference} interchangeably with \textit{transfer}. However, if the distinction is made, interference stands to mean \textit{negative transfer}, ``a gross deviation from the target language norm, or what could be called a translation error in one sense'', according to~\citet[p.71--72]{Mauranen2004}. Positive transfer is the reliance on the intersection of patterns found in both languages. In this work, we do not focus on this distinction, although our features attempt to capture differences in the use of intersecting patterns.

\citet{Chesterman2004} emphasised the influence of each member in the language pair by arranging the tendencies discussed in translationese studies into SL- and TL-universals depending on the type of comparison required to establish them. It is in line with one of the approaches to translationese, which describes linguistic properties of translations as `split' between normalisation and shining-through~\cite[see, for example, ][]{HansenSchirra2011}. \citet{Nikolaev2020} found that the relative contribution and prevalence of each tendency depended on the distance between the source and target languages. Translations from structurally-similar languages were found to demonstrate greater conformity to the TL norms and were more predictable, while translations from structurally-divergent SLs contained more non-idiomatic features making them more entropic and unusual in terms of lexical density, mean sentence length, frequencies of conjunctions and passives, etc. 
%frequency profiles of the list head~\cite{Laviosa1998}, i.e. Cumulative proportion of a about a hundred high-frequency words, Repetition rate of high frequency words, Ratio of high-to-low frequency words a~\gls{LM} trained on TL non-translations. 

It seems reasonable that, according to some authors, a reliable translationese study should involve an aligned parallel corpus and an interface with findings from contrastive analysis, which is not always the case. \citet{Evert2017} state: ``it is methodologically impossible to determine differences between translated and non-translated texts without comparing the realization of a feature in the matching source text'' (p.50). This stance is reminiscent of an early Toury's opinion that ``an exhaustive contrastive description of the languages involved is a precondition for any systematic study of translations''~\cite[p.29]{Toury1980}. When the methodological setup of a study complies with this requirement, a difference can be made between adaptation (covert translation, when diverging SL frequencies of a feature are fully adopted to the TL norm) and \textit{over-normalisation}, an exaggeration of target language norms leading to unusual overuse of typical TL patterns~\cite{Evert2017}. 

Finally, it is not unusual to group trends in translational behaviour into shining-trough and source-language-independent effects~\cite{Koppel2011} or SL/TL-independent translationese, i.e. ``forms of translationese that cannot be traced back to the respective source language or to individual source texts''~\cite[p.54]{Evert2017}.

On top of the four major trends, defined on page~\pageref{pg:major_trends}, and interference (alternatively called shining-through or transfer as shown above), \citet{Chesterman2004} mentions eight other self-explanatory claims such as \textit{lengthening}, \textit{reduction of repetition}, \textit{unique items hypothesis}, \textit{untypical lexical patterning}, etc.

In the current terminological conventions, these disparate phenomena are known as \textit{tendencies in translational behaviour}~\cite[see the use of the term in][]{Laviosa2008, Cappelle2017} that result in statistical differences between translations and comparable non-translated (i.e. originally-authored) texts in the target language.

\textbf{In this thesis translationese is broadly understood as any statistical deviations of translations from comparable non-translations in the target language}, regardless of their origin and nature. The main focus of our investigation is the correlation between the amount of translationese captured by the proposed feature sets and available quality labels. The identification of prominent translationese indicators and the analysis of translationese trends are viewed as preliminary steps. 

The original interpretation of the term \textit{translationese} by~\citet{Gellerstam1986} was purely descriptive and as such should be devoid of any derogatory connotations. According to him, translationese is not the result of poor translation; it is a statistical phenomenon, which consists in the diverging distribution of certain features.
We agree that diverging patterns in translations are their inherent inevitable property. However, a relative amount of translationese represented as distance from the expected standards (and measured in~\citet{Sutter2017}, for example) might have implications for quality. An in-depth account of theoretical and empirical foundations for this statement appears in Section~\ref{sec:feats4qua}. 

In the related literature, \textit{translationese} is occasionally used as an \hypertarget{wd:evaluative}{evaluative} term. For~\citet{Baker1993}, translationese describes ``cases, when an unusual distribution of features is clearly a result of the translator's inexperience or lack of competence in the target language'' (p.249). \citet{BlumKulka1986} distinguish `true hybrid' (as a result from positive translation solutions) and ``inadequate text which exhibits features of translationese, resulting from a lack of competence'' (p.167). 

Another conflicting use of the term \textit{translationese} is metonymic: in a number of contexts, especially in the publications in computational domains, \textit{translationese} is used interchangeably with \textit{translated texts}. Explanations like ``translationese can be defined as any text that has been translated into another language and thus bears certain linguistic features such as simplification, normalisation and explicitation'' are not uncommon. \citet{Nikolaev2020} clearly uses translationese to refer to translated texts: ``translationese as a special language form'', ``predictability of translationese'' ~\cite[see also  ``$\dots$ contains elements of both original text and translationese'' in ][p.52]{Carter2012}. 
In this thesis, \textit{translationese} is used to mean translationese properties of translations.
\citet{Popescu2011} used \textit{translationese identification} to refer to a binary text classification (translations vs. non-translations in the TL). Throughout this thesis, the terms \textit{translationese classification} or \textit{translation detection} are adopted to denote this experimental setup following a recent practice in computational translationese studies~\cite[see, for example,][]{Pylypenko2021}. 

\subsection{\label{ssec:feats}Features of Translation}
% what are features, and (linguistic) translationese features in particular
% what are the typical approaches to text representation in translationese studies
In this work, `features' are defined in line with the computational linguistics convention as quantitative characteristics of samples/observations (documents) used to represent these samples in a machine leaning setup or for statistical analysis. A feature is a numeric evaluation of a particular parameter of a sample. To put it simply, features are names of the (non-metadata) columns in a data table, where rows represent each sample/observation.
% A feature can be defined as a unique property or characteristic of a phenomenon that can be measured (Bishop, 2006)
However, in traditional translation studies the term \textit{(universal) features} can be used in a different meaning and context. For example, \citet{Zanettin2013} employs this term to refer to simplification, explicitation, etc (p.23), while practical frequency-based operationalisations of these hypotheses are called `formal operators'. 

%The related literature exhibits some terminological variation in this respect (features of translationese, features of translation, features of translated language, translationese indicators), but these variants seem to refer to the actual input to the machine analysis, which is often a multi-dimensional feature vector, i.e. a sequence of values. 

In some descriptions, a feature can refer to a multi-dimensional vector, and not one value. \citet{Puurtinen2003} investigates three features, each of which is actually associated with frequencies of several or many items: complex non-finite constructions, clause connectives, or keywords. While in~\citet{Puurtinen2003} each of thirteen connectives is investigated separately, \citet{Xiao2010}, inter alia, compares translated and non-translated corpora using cumulative normalised frequencies of dozens of listed conjunctions in each of 2,000-word text chunks in their study. Thus, in~\citet{Xiao2010} setup the same feature (connectives) would be represented by just one number for each sample, and in~\citet{Puurtinen2003} it is a multi-dimensional vector.
Multi-dimensional vectors made of values for homogeneous items (esp. frequencies of individual n-grams or POS) are commonly referred to as text representations rather than features~\cite[see usage in ][]{Baroni2005,Kurokawa2009}. 

The choice of features largely depends on the research methodology and research goal. A distinction can be made between corpus-based and machine learning computational paradigms, each with their own approaches to data collection, research design and feature selection. We overview these paradigms below.

\paragraph{Corpus-based and computational approaches} 
\label{pg:coling_begin}
\textbf{Corpus-based studies} rely on manual analysis of (parallel) concordances and statistical comparison of translational and reference corpora. By necessity they use a few well-picked linguistic forms or groups of them. 

For example, based on contrastive knowledge about grammatical structures in English and Portuguese, \citet{Santos1995} explores the use of tense and aspect verb forms in clearly defined translationally interesting types of contexts. These contexts inspire specific theoretical expectations of translational behaviour. 

A typical case is the overt use of linguistic devices that are expressed (and/or obligatory) in the \gls{ST} and optional in the \gls{TL}. 
Explicitation of TL-optional cohesive markers and changes in the frequencies of specific semantic groups of discourse markers has become a very fruitful topic for corpus-based translation studies~\cite{Olohan2001,Nakamura2007,Castagnoli2009,Becher2011,Kunilovskaya2017conn}. These studies demonstrated that cohesive markers were good indicators of translationese. As a result, cohesive markers/connectives/conjunction have been invariably included into more extended feature sets in translationese studies~\cite{Puurtinen2003,Rabadan2009,Xiao2010,Redelinghuys2015}, including in the ML contexts~\cite{Volansky2015}.
However, some caution is required with regard to this feature when studying translations from TED talks corpus, an easily available, reasonably large parallel corpus, spanning many language pairs. TED talks corpus contains translations produced for subtitling purposes. One of the instructions included into the guidelines for volunteer translators, helping them to meet the required length requirement, is to lose connectives\wlvfootnote{\url{https://translations.ted.com/How_to_Compress_Subtitles}}.  

In a similar vein, earlier corpus-based studies have established consistency of such translationese indicators as \gls{TTR}, lexical density, mean sentence and word length, frequency distributions of most frequent words~\cite[known as `list head' from][]{Laviosa1998}. These features were tested on various language pair, including English-to-Chinese~\cite{Xiao2010} and translated English~\cite{Kruger2012,Redelinghuys2015}.

Corpus-based approaches are characterised by acute interest to catching translationese on the lexical level. Apart from lexical profiles and distributions of words from various frequency bands, researchers compare counts of neologisms, loanwords, hapax legomena, cognates, formal vs. informal vocabulary, `common lexical bundles'~\cite{Kruger2010}, phrasal verbs~\cite{Cappelle2017} to study creativity and standardisation in translation.

Statistical methods used to reveal translationese trends include significance tests and effect-size coefficients, \gls{ANOVA}, measures of log-likelihood~\cite{Xiao2010} and $\chi$-squared~\cite{Cappelle2017}, correspondence analysis~\cite{Delaere2012}.

More sophisticated corpus techniques are used by~\citet{Sutter2017} and~\citet{Evert2017}. These studies are focused on visualisation as a research method and make use of unsupervised and mildly-supervised ML approaches, including \gls{PCA} and \gls{LDA}.

In terms of features, they build extended lists of well-motivated features, bringing together the previous findings in the field. Both studies make explicit use of language-specific features. 

\citet{Evert2017} set out to explore the impact of translation direction on the properties of translations using a register-balanced bidirectional English-German corpus. Their methodology combines corpus-based statistical analysis, factor analysis and text classification.

According to the authors, there are two possible experimental setups to achieve their goal: it is either access to the SL side of a parallel corpus or availability of translations from many SLs into the same target. They go with the former and make a point of employing cross-linguistically comparable features to be able to compare their frequencies in the SL, TL and in translations. 
The latter setup is very popular in translationese studies, too, but most of the work is done on the same corpus, EuroParl~\cite{Koehn2005} and its versions~\cite[to name just a few]{VanHalteren2008,Koppel2011,Carter2012,Volansky2012,Lembersky2013,Rabinovich2017}.

Feature selection in~\citet{Evert2017} is guided by contrastive analysis to ensure cross-linguistic comparability, and by variational linguistics. The authors treat translated registers as language varieties and expect the features to be fit for catching register distinctions in their textual data. Table~\ref{tab:everts} has a few examples from their feature set, counting 22 features.

\begin{longtable}[H]{p{2.3cm}|p{13cm}}
	%		\begin{tabular}
		\toprule

%\wlvtab{lc} {everts} {Examples of features proposed by~\citet{Evert2017}}  
%{
%	\toprule
	Feature & Description \\
	\midrule
	adja\_T & \specialcell{attributive adjectives, per no. of tokens\\English: all tokens tagged ``JJ.*'' (general adjective)\\German: all tokens tagged ``ADJA'' (attributive adjective)}\\%
	finites\_S & finite verbs, per no. of sentences all items receiving the tag  (chunk\_gf=``fin'') in the manual annotation\\%
	lexical.TTR & \specialcell{lexical type token ratio\\all lemmatized items tagged as noun, full verb, adjective and adverb \\over noun, full verb, adjective and adverb tokens per text}\\%
	modals\_V & \specialcell{modal verbs, per no. of verbs\\all items receiving the tag ``VM.*'' (modal verb)}\\%
	nominal\_T & \specialcell{nominalizations, per no. of tokens\\English: all tokens tagged ``N.*'' ending on \textit{-ion, -ism, -ment, -ness}\\German: all tokens tagged ``N.*'' and ending on \textit{-ung, -heit, -keit, -ismus}}\\%	
	pronouns\_T & \specialcell{personal pronouns, per no. of tokens\\listed items for each language tagged ``PP.*''/``PPER''}\\%
	subordin\_T  & \specialcell{subordinating conjunctions, per no. of tokens\\English: all tokens tagged ``CS.*''\\German: all tokens tagged ``KOU.*'' or ``KOKOM''}\\%
%	\bottomrule
%}
	\bottomrule
	
	\caption{\label{tab:everts}Examples of features proposed by~\citet{Evert2017}}\\
\end{longtable}

This PhD thesis generally follows the approach to feature selection described in~\citet{Evert2017}.
 
The same theoretical stand with regard to feature selection is consistently maintained in a series of publications by~\citet{Lapshinova2015,Lapshinova2017}, who uses a smaller set of simpler features, intersecting with Evert's and linked to components of communicative situation (field, tenor, mode) underlying register variation according to~\citet{Halliday1989}. In particular, their feature set includes (i) a subset of PoS n-grams (nominal and verbal chunks); (ii) nominalisations and general nouns; (iii) semantically categorised modal expressions; (iv) evaluative patterns capturing author's epistemic stance, along with other features such as lexical density and frequency of pronouns that are typically used in both translationese studies and register analysis.

\citet{Sutter2017} compares student and professional translations to non-translations using \gls{PCA} plots and \gls{ANOVA} analysis. The features in~\citet{Sutter2017} are much less linguistically-motivated, but most of them are easily-extractable and language-independent. The authors admit that the selection is rather arbitrary (p.29). \hypertarget{wd:sutters}{Their features} include a lot of surface phenomena such as basic frequency information on different PoS categories (lexico-grammatical features), TTR, lexical density, hapax legomena, Zipf-based general word-frequency measure and the total number of the 100 most frequent 3- and 4-grams, which reflects the degree of syntagmatic patterning. The statistical results demonstrate that only seven out of 30 features capture distinctions between students and professional (in English-to-French scenario).

A focus on student translations is not uncommon in corpus-based studies~\cite{Nakamura2007,Rabadan2009, Castagnoli2009,Sutter2017,Kunilovskaya2018profiles} and computational studies~\cite{Ilisei2010,Lapshinova2019,Bizzoni2021}. The findings often show that students exhibit more translationese than professionals. We will discuss this aspect of translationese studies in Section~\ref{ssec:pro}.

As shown above, the use of interpretable, theory-based features is typical for corpus-based approaches. Feature selection is often guided by cross-linguistic considerations~\cite[as in][]{Santos1995,Evert2017,Cappelle2017} and anticipated shining-through effect, or other from translation-process related theory~\cite{Redelinghuys2015,Redelinghuys2016,Delaere2015}. 

\textbf{More recent translationese studies moved from corpus-based approaches to computational methods}, and rely on supervised and unsupervised machine learning techniques. 
Depending on the research goal, \textbf{computational translationese studies} can be designed with the view to shed light on the nature of translational behaviour~\cite[see, for example, ][]{Corpas2008,Ilisei2010} or as a computational challenge seeking to solve an associated NLP task, e.g. translation detection and especially, SL detection, a task that has developed a lot of traction recently. For example, \citet{Sominsky2019} acknowledge: ``Our main goal here was to guarantee best performance, even the cost of interpretability'' (p.1134).

A good example of a ML-based translationese study is presented in~\citet{Volansky2015}. They state: ``Our main objective, however, is not to produce the best-performing classifiers. Rather, it is to understand what the classifiers can reveal about the nature of the differences between O [non-translations -- MK] and T [translations -- MK]'' (p.110). Their work has become influential in subsequent research and is often used as a baseline due to a clear description of the methodology, including extraction procedures for a fairly extended set of features. For each text in their experiment, they extract 38 features, with some being a single value (i.e. TTR) and other being multi-dimensional vectors (e.g. for POS trigrams they extract 300 most frequent sequences). Each feature is linked to one of the four translation universals (simplification, explicitation, normalisation and interference) in a top-down manner to explore the hypothesised trends in translational behaviour. Table~\ref{tab:volanskys} reproduces their mapping.

\begin{table}[!ht]
	\centering
	\begin{tabular}{lc}
	\toprule
	Category & Feature \\
	\midrule
	Simplification & \specialcell{TTR\\Mean word length\\Syllable ratio\\Lexical density\\Mean word rank\\Mean sentence length\\N most frequent words}\\ 
	\hline%
	Explicitation & \specialcell{Explicit naming\\Single naming\\Mean multiple naming\\Cohesive Markers}\\%
	\hline
	Normalisation & \specialcell{Repetitions\\Contractions\\Average PMI\\Threshold PMI}\\%
	\hline
	Interference & \specialcell{POS 1-,2-,3-grams\\Character 1-,2-,3-grams\\Prefixes and suffixes\\Contextual function words\\Positional token frequency}\\ \hline%
\end{tabular}
	\caption{\label{tab:volanskys} Features associated with each translation trend in~\citet{Volansky2015}}
\end{table}

It can be seen that the features in~\citet{Evert2017} are more knowledge-rich than the features in~\citet{Volansky2015}. The former implements fairly complex extraction rules to get nominalisations, for example. Naturally, more complex rules raise the reliability issues for the extraction procedures. These two studies use their feature sets differently: \citet{Volansky2015} tests each feature (or feature set) separately, while~\citet{Evert2017} argues for a multivariate analysis and uses all feature values as a feature vector representing each document in the experiment.   

%Volansky's feature set is actually representative of the theory-oriented strand of translationese research. Although we would argue that these features are fully interpretable, because each represents a particular linguistic phenomenon, they are more human-readable than the features employed in more task-minded setups, not to mention that in some cases researches devise complex ST-TT similarity features for the task that are more in line with quality estimation features. Note that representations used for translation quality estimation are presented in Section~\ref{sec:qe}.

%Building on the previous work by~\citet{Eetemadi2015}, \citet{Sominsky2019} proposed a new complex type of representation for solving the translation direction detection task at sentence-level. Their representation leverages word-level alignments to produce a vocabulary of lexically anchored minimal translation units inspired by phrase-based machine translation techniques. However, these features were outperformed by simpler representations such as function words or positional token frequencies.
%
%A few then-novel features, linked to simplification, appeared in~\citet{Ilisei2010}. On top of the standard features related to the distribution of grammatical classes of words and other measures of translationese that were discussed above, they calculated:
%\begin{enumerate}\compresslist{}
%	\item sentence depth as the parse tree depth;
%	\item proportion of simple sentences, complex sentences and sentences without any finite verb;
%	\item ambiguity as the average of senses per word.
%\end{enumerate} 

The second difference between these two studies is the underlying theory. While features in~\citet{Evert2017} rely on findings in contrastive linguistics and register studies, \citet{Volansky2015} select them to represent theoretical categories from translation studies in a top-down manner, by mapping each feature to a translationese trend. 

The quest for translationese trends, represented by pre-selected features, has been a very popular design in computational translationese studies~\cite[and most recently~\cite{Hu2021}]{Corpas2008,Ilisei2010,Volansky2015}.
These studies returned controversial results. To give some examples, \citet{Corpas2008} confirmed positive results for some features they associated with simplification, but not for the others. \citet{Kruger2010} reported limited support for the ``more explicit, more conservative, and simplified language use in the translation corpus'' (p.26), as represented by features selected by them. A recent study by \citet{Hu2021} did not find support for either simplification or normalisation.

This is not surprising for at least three major reasons: 
\begin{enumerate} \compresslist{}
	\item the mapping of particular features into descriptive translationese trends can be a matter of debate (as stated in \citet[p.25]{Zanettin2013});
	\item there can be differences in the extraction procedures;
	\item translations from different SLs and in different registers produce diverging translationese patterns.
\end{enumerate}

Therefore, in this research we refrain from assigning individual features (indicators) to particular trends such
as simplification and explicitation in a top-down manner. Instead, we follow a bottom-up approach and identify
the indicators of some translationese effects based on the similarity of their frequency pattern in
source texts, target texts and reference texts to categorise features as contributing to different translationese trends.
Our feature selection and research design is most in line with the third approach to translationese as presented in the beginning of Section~\ref{ssec:keyterms} (page~\pageref{ssec:keyterms}). 
We evaluate the features in the context of SL-TL contrast, i.e. as contributing to shining-through or normalisation tendencies.
\label{pg:coling_end}

\paragraph{Surface, linguistic and linguistically-motivated features} With regard to the amount of knowledge, informing the extraction procedures, the features used in translationese-related research fall into three categories. 

\textbf{Surface and `easily extractable' features} do not use any linguistic-theoretical information and/or external resources and are generated from raw text. Most commonly, they are character or word n-grams of various order. They can be interpreted at a higher linguistic level and are shown to capture morphological phenomena.
Surface features also include mean length of words or sentences as well as \gls{TTR}. 

It is well-known that character- and word n-grams, being lexicalised string features, tend to capture differences in topical domains between translations and non-translations, rather than true distinctions between respective language varieties. This limitation was described in very early computational studies, such as~\citet{Baroni2005} and \citet{VanHalteren2008}. Although surface features have been shown to deliver nearly perfect classification results~\cite{Popescu2011}, they do not generalise well across domains and capture uninteresting domain distinctions between corpora~\cite{Eetemadi2015}. Researches, interested in theoretical insights, prefer to opt for semi- or fully delexicalised features~\cite{Lapshinova2018,Nikolaev2020,Hu2021}. The same practice is adopted in experimental variational linguistics~\cite{Diwersy2014}.

\textbf{Linguistic features} are generated using external resources and language models. A very popular approach is \gls{PoS} n-gram representations (including frequencies of PoS tags, i.e. PoS unigrams) from annotated corpora which have all words replaced with their tags. 

In computational research designs, it is not uncommon to rely on simple linguistic features. For example, \citet{Koppel2011} used 300 functional words to achieve staggering classification results on a number of EuroParl language pairs, demonstrating the existence of SL-related ``translationese dialects''. \citet{Rabinovich2015} rely on function words and cohesive markers. \citet{Nisioi2013} use a set of \gls{MFW}, inspired by computational stylometry and authorship attribution studies.

One of the famous early studies by~\citet{Baroni2006} compares the performance of word n-grams and PoS n-grams, and their combinations on high-quality journalistic texts translated into Italian, using linear \gls{SVM}. They show that the best translationese indicators are distribution of function words and morphosyntactic categories, including personal pronouns and adverbs.

A fairly novel and well-motivated approach to linguistic feature selection relevant to this project is presented in~\citet{Hu2018}. They use delexicalised syntactic features -- constituent parse trees and dependency triples -- as features. Namely, they use frequencies of (i) each grammar rule extracted from the parse trees, (ii) `depTriple', i.e. PoS of a head and its dependent along with the dependency relation, and (iii) dependency labels. They found that combined feature sets work significantly better than one feature set alone, but translations could be reliably distinguished from non-translations on only five structural features. Their analysis of top-ranking features proves that syntactic representations yield themselves for interpretations better than PoS n-grams that were interpreted on the syntactic level in previous research.  

Recent research makes use of language modelling approaches. An important distinction to be made here is between feature-based and end-to-end feature-learning approaches. The main use-case scenarios are as follows. 

First, vector representations from pre-trained models can be used as input features in their own right (e.g. \citet{Pylypenko2021} uses \textit{fastText}\footnote{\url{fasttext.cc/docs/en/pretrained-vectors.html}} word vector models in an SVM classifier) or to initialise a neural network architecture setup to solve the task at hand. For example, \citet{Sominsky2019} used a \gls{BiLSTM} seeded with GloVe word embedding vectors~\footnote{\url{nlp.stanford.edu/projects/glove/}} for each sentence pair to solve a challenging task of sentence-level translation direction detection. It was shown to outperformed all feature-engineering approaches attempted in their research. 
A recent work by~\citet{Pylypenko2021} explores the performance of pre-trained neural word embeddings and end-to-end neural architectures (feature-learning approach) in a translationese classification task to show that they outperform hand-crafted features but capture different aspects of translated language. Based on their multilingual experiments, they conclude that while (deep) feature-learning approaches provide evidence for the existence of universal translationese specificity, hand-crafted features used in the experiment can hardly explain it. Interestingly, their \textit{hand-crafted} features include crude surface features (average word length) and features from other linguistic representation (unigram bag-of-PoS, n-gram frequency distribution). % a more appropriate name for tham is explicit features, perhaps. 

Second, language models can be used to generate information-theory-related `surprisal' features (language modelling features) such as log-probabilities and perplexities~\cite{Rubino2016,Bizzoni2021,Pylypenko2021} or entropies~\cite{Nikolaev2020,Hu2021}. This approach builds on the assumption that if translated texts are different from non-translations they should be more difficult to predict for a language model trained on the originally-authored texts in the TL. The language models can be trained on delexicalised corpora, i.e. on sequences of PoS or syntactic tags. It allows (i) to minimise domain-related differences and (ii) to reduce the model vocabulary making it possible to learn patterns on relatively small training corpora. \citet{Rubino2016} computed surprisal, complexity, and distortion features based on flattened constituent parse trees.
In~\citet{Bizzoni2021}, \gls{LM} perplexities were interpreted as a measure of translationese, but the results were controversial: the authors reported consistently higher perplexity on professional translations than on student translations across a number of registers. It means that their neural model trained on non-translations in the TL found professional translations more unusual than student translations.

% Rubino: speakers modulate the order, density and specificity of their expressions to avoid informational peaks and troughs that may result in inefficient communica- tion. This is often referred to as the uniform information density hypothesis (Frank and Jaeger, 2008). The information conveyed by an expression can be quantified by its surprisal, a measure of how predictable an expression is given its context.
% Source language interference should result in peaks of measured surprisal val- ues in translated texts, while the information density may remain uniform in originals.

Third, \citet{Chowdhury2020,Chowdhury2021} solve a computational typology task of reconstructing phylogenetic language trees from translations, first attempted by~\citet{Rabinovich2017}. Following the approach in~\citet{Bjerva2019}, they generated embeddings for the whole language varieties (or, in terminology offered in~\cite{Koppel2011}, translationese dialects, i.e. translations from many languages into one language) and measured distances between these embedding spaces. The models were trained on delixicalised versions of originally-authored English and English translated from a dozen of languages to demonstrate that translations retained enough traces of respective SLs to reproduce linguistically-informed language typology. As expected, translations from SLs belonging to the same language family clustered together, and the vector spaces for structurally similar SL and TL had more isomorphism. 

\textbf{Theoretically-motivated or linguistically-motivated features} usually stand to mean features which are designed to capture specific linguistic phenomena. They are usually backed by findings from contrastive linguistics or from translation studies, and require relatively complex extraction rules and external resources, including pre-defined lists of items and additional corpus annotation. Many of these features were presented in the paragraphs on corpus-based approaches in translation studies (pages \pageref{pg:coling_begin}-\pageref{pg:coling_end}).

%\todo[inline]{preliminary}
This research uses a set of explicit linguistically-motivated and interpretable features, which are tested to capture translationese in student and professional translations, in a quality estimation task. 
To demonstrate their competitive performance, we compare them with \gls{tf-idf} weighted word n-grams, on the one hand, and with representations from pre-trained embedding models, on the other, for both tasks. The description of our selection of features appears in Section~\ref{sec:myfeats}.

\section{\label{sec:feats4qua}Translationese and Quality}
This section presents the rationale behind the hypothesis put forward in this research. What theoretical and empirical evidence is there to assume that translationese features might be correlated with translation quality? 
 
The link between translationese and translation quality has been assumed in corpus-based translation studies ever since translationese has become an active research field. It is typical to use the term \textit{translationese} in \hyperlink{wd:evaluative}{derogatory contexts}, similar to other words built after this derivational pattern: \textit{legalese, journalese, academese, post-editese}. All these nominations imply hyperbolic, unnecessarily exaggerated use of features characteristic to the respective communicative situation, often opposed to plain language: legal matters, news, academic writing, etc. % e.g. unnecessary jargon associated with the field of academia, particularly common in academic writing in humanities, and the opposite of plain language 

Although translations are always different from non-translations bearing inevitable marks of the translation process, the magnitude of these differences can be related to quality.

\paragraph{Evidence from previous studies} There is a number of corpus-based studies that correlate the individual translationese trends and quality scores/translation expertise\footnote{Note that some researchers exercise caution in using professional varieties of translation (students and professionals) as a proxy for quality}.

In particular, linguistic features linked to simplification and explicitation were tested as possible indicators of translation quality with a view to improving teaching methods and assessment criteria at postgraduate level. Simplification was found to correlate with lower-scoring translations and explicitation was found to correlate with higher-scoring translations \cite{Scarpa2006}. 

\citet{Rabadan2009} interpreted translationese departures from TL norms as quality-related. They identified three grammatical translationese-prone areas in English-to-Spanish translation (quantifiers, modifiers of nouns and the translation of the English Simple Past form) and demonstrated how to use corpora to corroborate disparities between translated language and non-translations. % : ``published translated materials frequently show grammatical uses that turn the text difficult to understand or even partially meaningless in the target language (TL), causing a deficient flow of the text and a perception of overall low quality'' (p.303).
Interestingly, they called for an implementation of language-pair-specific statements as \gls{TQA} tools that could contribute to the systematisation and objectivisation of translation assessment. Consider the examples of the proposed conditional statements for English-to-Spanish translation:

\begin{enumerate}\compresslist{}
	\item The lower the number of formal options chosen from those available in Spanish to translate intensified quantification, the higher the degree of simplification and the less accurate the translation, and vice versa;
	\item The lower the number of \textit{de-}phrases, the higher the number of pre-modifying adjectives in translated Spanish, the higher the degree of interference and the less idiomatic the translation, and vice versa;
	\item The smaller the disparity between native and translated usage in the use of particular grammatical structures associated with specific meanings, the higher the translation rates for quality.
\end{enumerate}

\citet{Redelinghuys2015} and \citet{Redelinghuys2016} explored the differences between professional translators and students with regard to hypothesised trends in translational behaviour (explicitation, simplification and normalisation, levelling-out). The authors avoided referring to levels of expertise as quality categories. Instead they talked about using ``features of translationese as indicators of translation expertise''~\cite[p.192]{Redelinghuys2016}. 

Unlike the previous work, \citet{Sutter2017} treated student and professional translations as quality-related categories. The documents in their corpus-based study were represented as feature vectors valued on typical \hyperlink{wd:sutters}{translationese indicators} from previous research. They visually explored locations of individual translations relative to professional standards (professional translations and comparable non-translations) in a low-dimensional space and performed univariate analysis to produce aggregate judgments about deviations in student translations. However, no predictive model was attempted. They also reported conflicting results on their two case studies attributing the confusion to feature selection and calling for less shallow, more linguistically-motivated approach. They highlighted that translationese-based approach to quality reflected only the fluency aspect of quality.

At the onset of machine learning approach to translation detection, \citet{Baroni2006} briefly mentioned developing an automatic translationese spotter to be used in translator education. 

In a ML study based on a register-balanced English-German corpora of human and machine translation, \citet{Lapshinova2015} found that human professional translations might deviate from the conventions of non-translated TL registers, and therefore, could not be relied upon as the only gold standard for MT quality evaluation: TL comparable texts belonging to the same registers should also be considered. 

The most straightforward application of translationese indicators to translation quality estimation task is reported by~\citet{Aharoni2014} in the field of statistical MT. They proposed a generic machine translation quality estimation technique which used the accuracy metric from a translation detection task. The result of the study indicated that translation detection accuracy was strongly correlated with the quality scores, yielding $R^2$ = 0.774 (in the experiment using human evaluation based on pairwise sentence rankings from WMT13 dataset). The ratio of sentence pairs identically ordered by the proposed metric and human evaluators (based on French-English data from \gls{WMT}-2013) was 0.846. Quite unexpectedly, they used binary features, denoting the presence or absence of PoS n-grams and each of 467 function words.

The intimate link between translationese and quality is manifested in a computational study that solves the reverse task of predicting translations using \gls{MTQE} features. In other words, the authors used indicators of translation quality to predict student and professional translations~\cite{Rubino2016}.

\paragraph{Fluency as overall quality?}
Generally, it seems reasonable to contend that the smaller the disparity between originally-authored and translated language, the higher the translation quality.

It is obvious that translationese-related defects reflect only one aspect of quality, namely, fluency. The analysis of pragmatic and semantic relations between ST and TT is not involved in translationese-related experiments rendering acceptability and accuracy inaccessible for translationese-based measures.

It can be argued that fluency is a major quality factor in human perception. Grammatical disfluencies and raptures to the natural text flow, which sometimes turn translations meaningless, immediately catch an eye and have an impact on the overall perception. Even if translationese does not result in ungrammatical structures, it generates strange and unnatural linguistic forms that are recognised as literal translations by professional bilinguals and by general public. `Does not sound right' effect in translations can be linked to the concept of non-binary translation errors put forward by \citet{Pym1992}. Repeated manifestations of translationese can be considered a distributed translation error, which can only be diagnosed based on document-level statistical analysis like the one proposed by~\citet{Sutter2017}. 

The strong preference for fluency in translation assessment can be explained by the current dominance of translator's invisibility as a conventional norm in translation as opposed to a more marginal strategy of `foreignisation'~\cite{Venuti1995}. 
Although it has been shown that professional tolerance to interference might vary across registers in one language and across language pairs (language prestige effect), the general expectation for a high-quality professional translation is to be covert and as much blended with the TL norm as possible. 

At the same time, it is obvious that semantic relations between ST and TT cannot be entirely ignored in translation quality estimation. Accuracy might be a more important aspect of quality than fluency: it is better to have a poorly-written translation that accurately conveys the content of the ST than a very fluent translation that misinforms the reader about the meaning of the source. 
However, when it comes to evaluation experiments, accuracy-fluency distinction might be more theoretical than practical. Human annotators of MT produced highly correlated scores for accuracy and fluency~\cite{CallisonBurch2007}, and there is a trend to dispense with this distinction in the annotation setups acknowledged in a recent survey of the MT evaluation metrics~\cite{Chatzikoumi2020}. Also, this problem is recognised in~\citet{Daems2017}: their solution was to keep the annotation of two aspects of quality separate.

Besides, it is difficult to imagine a human translation produced in good faith that would completely and seamlessly misrepresents the content of the source. This last argument can be supported by the results obtained in the studies on student translations. \citet{Carl2010} found that the major differences between learner and professional translations were about the degree of fluency. \citet{Sutter2017} quotes~\citet{Loock2016} as concluding: ``some deviant linguistic characteristics of translations written by advanced students might be symptomatic of the overall quality of their translations'' (p.28). 

The error statistics in the \gls{RusLTC}\wlvfootnote{\url{www.rus-ltc.org/static/html/about.html}}, used as a source of student translations in this thesis, indicates that only 23.7\% of errors (less than a fourth) were content (or accuracy errors), while the majority of annotated translation solutions had to do with style, cohesion, lexis or sentence structure, i.e. with fluency. 
%\vspace{-1em}
\paragraph{Relevance of translationese for MT} The impact of translationese on the outcomes of MT has been demonstrated with regard to the training data, and with regard to the quality of test sets. 
A series of papers have demonstrated that it was important to take into account the directionality of translation when training MT systems\footnote{using all available language pair data regardless the translation direction is typical to increase the size of the training set}~\cite{Kurokawa2009,Lembersky2012,Lembersky2013,Stymne2017}.

Translational deviations from the target language were shown to have an impact on the reliability of MT evaluation. The systems demonstrated better results when test sets included reversed parallel sentences~\citep{Zhang2019, Graham2020}. Translated documents in the test sets proved less challenging for MT engines.

%\cite{Bizzoni2020} How Human is Machine Translationese?

\section{\label{sec:myfeats}Proposed Features and Representations}

\subsection{\label{ssec:select}Feature Selection}
In choosing features to test our hypothesis, we opted for \textbf{indicators of translationese that are at the same time `usual suspects' for human translation fluency errors} like it was done by~\citet{Rabadan2009}.

Practical textbooks for translation students typically address translationese-prone areas hypothesised from cross-linguistic contrasts, including contrasts in the frequency of shared features, and by observations of typical suboptimal solutions by inexperienced translators. 
An analysis of a representative sample of 46 translation textbooks described in~\citet[see Table 4]{Kunilovskaya2022err} demonstrated that most of the anticipated translation problems were grammatical phenomena (25 issues in 39 books). Lexical and stylistic issues attracted less attention in the examined publications (16 issues in 31 books). However, the analysis of error-annotated student translations revealed higher salience of lexical problems, as well as a different ranking for more frequent grammar difficulties. 

Taking into account the focus in the textbooks corrected with the analysis of actual errors observed in real-life student translations in our corpus, as well as considerations from the previous corpus-based and computational research on translationese, described above, we propose a set of 60 morphosyntactic features~(Section~\ref{ssec:ud}) and two sets of abstract lexical features: 24 collocational features~(Section~\ref{ssec:coll}) and 10 features from n-gram lists and n-gram language models~(Section~\ref{ssec:ngram}).

Various components and versions of these feature sets were used to analyse English-to-Russian and English-to-German translation in four registers~\cite{Kunilovskaya2020vars}, to explore its robustness to register variation in English-to-Russian professional translation~\cite{Kunilovskaya2021regs} and to solve a SL detection task based on literary translations into Russian from 11 languages~\cite{Kunilovskaya2021ranlp}.

This section starts with a few examples of translationese-related problems observed in student translations.

Typical translationese-prone areas for English-to-Russian translation include such morphosyntactic categories as the overuse of relative clauses, copula verbs, modal predicates, analytical passives, generic nouns, all types of pronouns, etc. Consider the following examples:

%===========
%\begin{examples}
%	Necklaces, at rst as pectorals that covered the whole chest, evolved from the
%	prehistoric pendants. 
%	\textcyrillic{Oepeeepoe apyoe ypaee, oopoe
%		aao eco a ce py, oopoe cao ocoo  oeco}
%	[Necklacerst chest decoration, which covered the whole chest, which
%	became the basis for pendants].
%\end{examples}

\ex. \label{ex:functionw}\hspace{1pt}
Necklaces, at rst as pectorals that covered the whole chest, evolved from the prehistoric pendants.\\
\textcyrillic{Oepee -- epoe apyoe ypaee, \textit{oopoe} aao eco a ce py, \textit{oopoe} cao ocoo  oeco} [Necklace -- rst chest decoration, \textit{which} covered the whole chest, \textit{which} became the basis for pendants.]

\ex. \label{ex:generic}\hspace{1pt}
\dots there are many self-employed people who manage to get money from others by means
of falsely pretending to provide them with some benefit or service \dots \\
\textcyrillic{ , \textit{}  \textit{},   , \textit{}    }  \dots [Moreover, many \textit{people} are, working for themselves, \textit{who} get the money in a deceitful  \dots way].

\ex. \label{ex:modalp}\hspace{1pt}
\dots differences in self-efficacy may simply mean that some teachers struggle to identify
solutions to problems beyond their circle of control. \\
\dots \textcyrillic{   \textit{}   \textit{, }           \textit{, }  \textit{} .} [\dots difference in self-evaluation \textit{can} mean only \textit{that that} some teachers run into difficulties in finding solutions to tasks beyond the scope of \textit{that what} they \textit{can} control.]

\ex. \label{ex:copula}\hspace{1pt}
It was difficult and exhausting to see. \\
\textcyrillic{\textit{ }     .} [\textit{It was} hard and exhausting to try to see.]

\noindent These examples from RusLTC demonstrate a number of translation solutions that explain the
increase in the frequency of TL items that are less frequent in non-translated TL
than their literal counterparts in the SL.

Probably, none of the translations in the examples can be considered ungrammatical in Russian, but there
is a clear Master Yoda-style foreign sound to them. Beware that the back translations may come across as perfectly acceptable sentences, because the translations are very literal in the first place.

In Example~\cref{ex:generic} a generic noun (\textit{people}) is rendered with a less frequent literal \textcyrillic{\textit{}}, instead of a possible structure with zero subject or other more acceptable ways of expressing unspecified subjects. 

English and Russian have contrastive ways of expressing subjective modality: modal verbs are a less common choice in non-translated Russian, which prefers parenthetical epistemic markers. The translation solution in Example~\cref{ex:modalp} carries over a typical English modal predicate. 

Example~\cref{ex:copula} has a notorious literal rendering of a structure with introductory \textit{it} and a copula verb in a context, where zero copula is typical in Russian. This contributes to a boost in frequencies of pronouns and copula verbs in translated Russian. Besides, such renditions have a strange word order, which usually interferes with the smooth flow of information in the text.

Another source of surplus function words, including pronouns, is the tendency to unpack the information from various concise English structures using strings of relative clauses, instead of repackaging the information in a more natural way (see Examples~\cref{ex:functionw} and ~\cref{ex:generic}). 

Below, we discuss principles and criteria that guided feature selection in this work, followed by a description of extraction procedures. 

Special effort was made to keep the features \textbf{cross-linguistically comparable} as suggested by~\citet{Evert2017}. 
While implementing extraction rules for a number of features we had to take into account diverging annotation rules and a variety of expression for a given category in either English or Russian. For example, English passive verb form corresponds to at least four Russian lexico-grammatical structures. 

The rationale behind this decision is an attempt to reveal the most notorious effect in translation, namely, shining-through, a translational tendency to reproduce SL patterns and frequencies rather than follow the TL conventions. This form of translationese can be established by comparing the distributions of feature values between English source texts, translations into Russian and comparable Russian non-translations. Besides, this allows to put the three corpora into a shared feature space to explore the  distances between them visually.

Cross-linguistic analysis yields interesting contrastive findings. Languages differ not only in what they have to express or do not express, but, more importantly from a translational perspective, how they distribute comparable isomorphic features. For example, a study on semantic groups of connectives revealed diverging preferences in English and Russian with regard to marking relations between discourse units. 
In previous research, it was shown that English had a tendency to mark contrast (\textit{however, but, nonetheless}) more often than Russian, while Russian had more of explicit markers of inference (\textit{therefore, because, that is why})~\cite{Kunilovskaya2017conn}.   

As it was shown above, we view translations as a TL subsystem, and, therefore, we explored approaches to computational representations used in variational linguistics to find a broad intersection between features from variational linguistics and features from translationese studies. 
In particular, we considered features and extraction procedures suggested by \citet{Biber1995,Nini2015} (for English) and~\citet{Katinskaya2015} (for Russian). Note that we had to limit ourselves to the phenomena that are \textbf{reliably extractable}, among other considerations. While the extraction of abstract lexical features is straightforward, the rules for morphosyntactic features were manually double-checked on our corpora to ensure that they return the expected results. The accuracy of feature extraction is largely dependent on the accuracy of the automatic annotation. Care has been taken to filter out noise by using empirically-motivated sets of function words; recurrent annotation errors were taken into account where possible. More details on annotation and extraction are given in Section~\ref{ssec:ud}.

We also took into account the general requirements to investigation of translationese, formulated by~\citet{Volansky2015} and reinforced in subsequent research: features should reflect \textbf{frequent} linguistic characteristics, be \textbf{content-independent} (delexicalised) and be \textbf{interpretable}. Our purpose is to understand translational behaviour better by verifying existing intuitions and possibly uncovering new trends in student translations affecting perceived translation quality. 

Finally, an attempt was made to reduce collinearity of morphosyntactic features and avoid features that go together in most cases. For example, coordinating conjunction (\textit{cc}) is a syntactic relation that is assigned to items with the PoS tag \textit{CCONJ}; either \textit{cc} or \textit{CCONJ} should be extracted. The syntactic relation named \textit{conjunct (conj)} might be overlapping with \textit{cc} to an extent, but it also accounts for elements coordinated with a punctuation mark (a comma).
At the same time we included a few features that might lack a theoretical or empirical support at the moment, hoping for new insights to be obtained in a bottom-up matter.

%Each text instance in our corpus is represented as a sparse feature vector consisting of 94 components. 
To explore the competitiveness of these features, we compare their performance with other types of frequency-based features -- \textit{tf-idf} representations and features from \textit{QuEst++} -- and with embeddings from pre-trained neural models.

\subsection{\label{ssec:ud}Universal Dependencies-based Features}

The first subset includes 60 features extracted from \gls{UD} annotations (UD features). They reflect morphological and syntactic structure of language.
The Universal Dependencies framework~\cite{Straka2017} is selected for annotation in this project because it offers consistent annotation of similar constructions across languages and uses universal tags for structural categories based on typological studies that allow to bring out ``cross-linguistic parallelism across languages and language families''\footnote{\url{universaldependencies.org/introduction.html}}. Further details on annotation and preprocessing are given in Section~\ref{ssec:prepro}. 

The values for UD features are either normalised frequencies of various UD tags and their combinations or measures/ratios and averaged values derived from syntactic trees. 

In this section we offer two views of the UD-based feature set: 
\begin{enumerate}\compresslist{}
	\item with regard to the type of linguistic phenomena each feature represents (see Table~\ref{tab:feats}), and 
	\item with regard to the amount of engineering effort and manual intervention required to obtain feature values. 
\end{enumerate}

Appendix~\ref{appx:ud} has a full alphabetic listing of features by their shorthand names (codes) and more details on the extraction for each feature. Where possible, we provide references to relevant previous work apart from what was discussed above, to explain why specific features were included in the feature set. 

Types of general linguistic categories reflected by the proposed morphosyntactic features are presented in Table~\ref{tab:feats}. 

\begin{longtable}[H]{@{} l|l|c|p{10cm} @{}}
	\toprule
		& type & number & list of features \\
		\midrule
		% pasttense infs pverbals deverbals finites comp sup passives
		1 & word forms & 8 & past tense, passive voice form, finite and two non-finite forms of verb (infinitive and all participles), deverbal nouns, superlative and comparative degrees of comparison \\
		
		% ppron possdet indef demdets mquantif cconj sconj nn apd
		2 & word classes & 9 & nouns, personal, indefinite/total, possessive and demonstrative pronouns, adverbial quantifiers, coordinate and subordinate conjunctions, adpositions \\
		
		% addit advers caus tempseq epist
		3 & discourse markers & 5 & contrastive, additive, causative-consecutive, temporal-sequential connectives and epistemic stance markers \\
		
		% acl advcl act:relcl ccomp xcomp parataxis neg interrog
		4 & types of clauses & 8 & adjectival clauses and relative clauses as a separate feature, adverbial clauses, clausal complements with or without own subjects (\hyperlink{ft:ccomp}{ccomp} and \hyperlink{ft:xcomp}{xcomp}, respectively), asyndetically joined elements in a sentence, negative and interrogative sentences \\
		
		%  attrib advmod amod mpred nnargs aux aux:pass copula appos case conj mark iobj compound flat fixed obj obl nsubj nmod nummod
		% mquantifiers discourse
		5 & dependencies & 22 & adjective/participles and other words in attributive function (\hyperlink{ft:attrib}{attrib} and \hyperlink{ft:amod}{amod}, respectively), modal predicates, auxiliary verbs, passive voice auxiliary, copula verbs, appositional modifier, etc.\\
		
		% lexdens lexTTR mhd mdd numcls simple sentlength wlength
		6 & text measures & 8 & lexical type-to-token ratio and lexical density (based on disambiguated content types), mean hierarchical and mean dependency distances, number of simple sentences, number of clauses per sentence, sentence and word length \\
		\midrule
		& TOTAL & 60 & \\
		\bottomrule

	\caption{\label{tab:feats} Types of linguistic information by language level captured with UD features}\\
\end{longtable}

Rows 1-3 in Table~\ref{tab:feats} list morphological features and classes of words.

We extract \textit{eight grammatical and lexico-grammatical forms}. They include four features designed to capture degrees of nominalisation (finite form > participle > infinitive > deverbal noun). The respective `deverbalisation scale' was suggested by~\cite{Korzen2011}, who explored cross-linguistic differences between Italian and Danish, and their implications for translational behaviour. Similar differences in information packaging are discussed in connection with sentence splitting in language pairs with German and English or Norwegian~\cite{FabriciusHansen1999}. 
In English-Russian language pair there are two prominent translational tendencies in this respect. Russian is known to have a stronger preference for less verbal structures in informational and more formal registers. This feature is often overemphasised (over-normalised) in translation leading to unnatural use of deverbals, often ridiculed in the translation textbooks (e.g. \textcyrillic{\textit{      ,    .}} [Drawing of a line by him happens with the diligence of a student sticking out tongue from the effort.]) This might lead to the increased frequencies of deverbal nouns in light verb constructions (compared to Russian non-translations). On the other hand, the tendency to copy English syntactic structures (interference) results in carrying over sentences with finite clauses where a nominalisation transformation can be expected. Due to this, we can expect higher frequencies of finite verbs and more clauses per sentence. For example, it can be predicted that translation students would keep the adverbial clause in \textit{The first theme, on how data and theory interact, interested me the most.} to get something like \textcyrillic{\textit{   ,     ,    .}} [First topic about that, how data and theory interact, interested me most of all] instead of an arguably more natural rendition: \textcyrillic{\textit{  o       }} [First topic on interaction of facts and theory interested me the most].   
The necessity to account for free translator's choice explains why we exclude cases of obligatory use of infinitive forms after modal predicates from the counts.

The frequency of PoS unigrams, especially for \textit{functional word classes}, is a known and strong translationese indicator confirmed in a number of studies. Our focus is on four types of pronouns and on two types of conjunctions. 
The rationale for English-Russian language pair includes the following factors: 
\begin{enumerate}\compresslist{}
	\item Russian is a pro-drop language, while English is not. Personal pronouns are more frequent in English, especially in the descriptions of human poses and actions (e.g. \textit{she smiled through her tears}, or \textit{every member of our modern society}), where they are not usual in Russian. Hence, we expect more personal pronouns in English-to-Russian translations. 
	\item Diverging use of possessive pronouns and pronominalised determiners (e.g. this/that, any, every) is a common problem for novice and amateur translators. It is rooted in the drastic differences between English and Russian in how they organise the flow of information in the text (`communicative dynamism'~\cite{Firbas1992}) and signal the distinction between topic and comment (and/or theme and rheme). While English relies on the category of determination to a great extent, Russian mostly employs word order as means to convey the functional sentence perspective. This issue invariably attracts attention of translation scholars for similar language pairs: Mona Baker has a chapter on thematic and information structures in her practical translation textbook~\cite{Baker2011}.
	\item The expected increased number of conjunctions can be viewed as collateral to the expected higher frequency of clauses per sentence and increased sentence lengths. Inexperienced translators are known to refrain from sentence splitting; they tend to unpackage information from condensed English structures in a very linear manner instead of re-arranging it in a more TL-agreeable way.  
\end{enumerate}

For the purposes of this project, \textit{\gls{DM}} are defined as a broad heterogeneous category that includes elements that convey additional meaning to the content of a given discourse unit (a clause, a sentence, a paragraph). According to~\citet{Fraser2006} they are ``part of a discourse segment but are not part of the propositional content of the message conveyed, and they do not contribute to the meaning of the proposition per se'' (p.189). They fall into two major types: connectives, i.e. items that express semantic relations between discourse units and pragmatic markers conveying speaker's attitudes or arranging speaker-hearer interaction. 

Connectives are usually classified (with possible variation in terminology) according to type of relation into additive (e.g. \textit{in particular, such as}), causative-consecutive (e.g. \textit{as a result, after all}), adversative (e.g.\textit{instead, despite}) and temporal-sequential (e.g. \textit{eventually, meanwhile}). 
Pragmatic markers can reflect a much broader scope of pragmatic meanings, but most authors identify the author's epistemic stance markers (reflecting the speakers certainty of the truth value of the proposition) as the most important group~\cite[see][]{Fraser2006,Halliday1989,Biber1999}. In translationese studies based on variational linguistics they are referred to as `evaluative patterns'~\cite{Lapshinova2017} and include parenthetical adverbs or phrases like \textit{perhaps, of course} and \textcyrillic{\textit{a o , o, oaa}}.

As has been shown above, discourse markers, especially connectives, are a attractive research object in translationese studies: they are content-independent and structurally optional items. Translators are free to choose between explicit and implicit renditions, and their lexical choices are indicative of the adopted global textual and translational strategy.

This study explores five categories of DMs, each represented by their cumulative frequencies extracted by matching elements from pre-defined lists, counting 314 items for English and 269 items for Russian. More examples of each semantic group and their sizes are given in Appendix~\ref{appx:markers}.

In this part of feature design, we rely on the previous research~\cite{Kunilovskaya2017conn}, which reports contrasts between English and Russian as to the preferences in explicit marking of semantic relations between discourse units. Based on those findings, as well as on previous translationese research, we can expect higher frequencies of all discourse markers, especially adversative and epistemic markers.

The lists were initially produced independently from grammar reference books, dictionaries of function words and relevant research papers. The lists from~\citet{Biber1999,Fraser2006,Liu2008} and from ~\citet{Shvedova1980,Priyatkina2001,Novikova2008} were used for English and Russian to name just a few sources for each language, respectively. After the initial selection, the lists were verified for comparability. Orthographic and punctuation constraints were used for disambiguation. The output of the extraction procedure was manually checked to exclude greedy matching.

Almost half of all features in this feature set capture various \textit{types of syntactic phenomena} (rows 4-5 in Table~\ref{tab:feats}). We present types of clauses and sentences as a separate subgroup for convenience only: most of syntactic features are extracted using default UD dependency tags. Recent translationese studies demonstrate that syntactic features might be more informative than morphology in translation detection~\cite{Hu2021} and in SL identification task~\cite{Chowdhury2020}.  

Finally, we included \textit{measures of text complexity} (row 6 in Table~\ref{tab:feats}), typical in translationese research such as \gls{TTR}, lexical density, two measures of parse tree depth, number of clauses per sentence, sentence length, etc. 

The normalisation basis used to produce mean values across all sentences in a document varies depending on the type of phenomena. Following~\citet{Evert2017}, raw counts are normalised as follows:
\begin{itemize}
	\item all morphological categories (word classes) per total tokens in the document (9 features: nouns, four groups of pronoun, two groups of conjunctions, adpositions, adverbial quantifiers);
	\item all syntactic functions and sentence-level phenomena and measures of complexity per number of sentences (43 features: five groups of DMs, eight types of clauses/sentences, eight text measures, 22 dependencies);
	\item all verbal forms per number of verbs (6 features: passive, past tense, deverbals, infinitives, participles, finite forms);
	\item all degrees of comparison per total number of adjectives and adverbs
\end{itemize}

We used various approaches to extraction, depending on the nature of targeted phenomena and information available from UD annotation for each language.
\begin{enumerate}\compresslist{}
	\item frequencies of all items on pre-defined lists, allowing for some variation in surface form, and disambiguated using position in the sentence and punctuation (6 features), including cumulative frequencies of each semantic group of connectives (\textit{additive, adversative, causative, temposal/sequencial}), \textit{discourse markers of epistemic stance and adverbial quantifiers};
	
	\item frequencies for combinations of PoS and dependency tags (3 features), including \textit{ratio of nouns and proper names as verb arguments, adjectives/participles in attributive function} and \textit{pronominal determiners};
	
	\item frequencies of PoS tags, morphological features and/or syntactic tags (and their combinations) filtered on closed lists to filter out noise (8 features), including \textit{adpositions, coordinaive conjunctions, subordinative conjunctions, negative particles or pronouns, possessive pronouns, personal pronouns, indefinite and total pronouns};
	
	\item frequencies extracted by more complex rules, which are created to enhance cross-linguistic comparability based on the analysis of parallel concordances. They take into account diverging annotation conventions in two languages. These rules extract 13 features: 
	\begin{itemize}\compresslist{}
		\item comparative and superlative degrees of comparison,
		\item copula verbs (counts for \textit{there + be} were excluded), 
		\item interrogative sentences (sentences ending in \textit{?!} were included),
		\item indefinite and total pronouns (regular expressions are used to extract Russian pronouns; no lexico-grammatic annotation is available for these types of pronouns for either English of Russian unlike other languages in the UD project); 
		\item deverbal nouns (empirically-motivated negative and positive filters were used);
		\item passives (custom-made rules included Russian semantic passive constructions, such as \textcyrillic{\textit{  .}} [I-accusative here they-respect.] \textit{I am respected here.}) % no way to account for Voice=Mid in The book sells well
		\item participles and participial constructions, which have diverging annotation rules in English and Russian 
		\item infinitives: English rules for `bare' infinitive were taken into consideration: occurences after model verbs were excluded;
		\item modal predicates, which required rules and support list to account for cross-linguistic differences in expressing modality in the predicate;
		\item cumulative frequency of clauses per sentence;
		\item frequency of simple sentences;
	\end{itemize}

	\item ratios based on counts of specified tags, including lexical density, lexical TTR;
	
	\item means for sentence length and word length across all sentences in the document; 
	
	\item features measuring syntactic tree complexity (2 features): mean hierarchical distance and mean dependency distance;
	
	\item frequencies of a single morphological attribute or PoS tag (3 features): finite verb forms, past-tense verb form, all nouns 
	
	\item frequencies of 24 dependency tags out of 40 available in the UD annotation (including language specific variants available for both languages in this project) scheme\footnote{\url{universaldependencies.org/u/dep/index.html}}:
	% used dependencies: 'acl', 'acl:relcl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'aux:pass', 'case', 'conj', 'ccomp', 'compound', 'discourse', 'fixed', 'flat', 'iobj', 'mark', 'nmod', 'nsubj', 'nummod', 'obj', 'obl', 'parataxis', 'xcomp'
	\begin{itemize}
		\item six dependencies were dropped because they duplicated the custom extractors or were considered less informative (\textit{cop, cc, csubj, root, det, punct});
		\item ten dependencies were excluded because that returned monotone values (all zeroes) for some subcorpora or were very rare (\textit{reparandum, vocative, dislocated, clf, goeswith, dep, expl, list, expl, orphan})
	\end{itemize} 
	
\end{enumerate}

\subsection{\label{ssec:coll}Collocations}
% '%bigram-npmi>0', '%bigram-npmi<0', '%bigram-npmi>0.5', '%bigram-npmi_absent', 'av_bigram-npmi>0', 'bigram-npmi_std', '%bigram-tscore>0', '%bigram-tscore<0', '%bigram-tscore>6.0', '%bigram-tscore_absent', 'av_bigram-tscore>0', 'bigram-tscore_std', 

% '%trigram-npmi>0', '%trigram-npmi<0', '%trigram-npmi>0.5', '%trigram-npmi_absent', 'av_trigram-npmi>0', 'trigram-npmi_std', '%trigram-tscore>0', '%trigram-tscore<0', '%trigram-tscore>6.0', '%trigram-tscore_absent', 'av_trigram-tscore>0', 'trigram-tscore_std'

Over 30\% of the translation errors annotated in student translations fall into the lexical category (see details on the error typology and annotation process in Section~\ref{ssec:err}). Over half of the issues types revealed through manual analysis of student translations are caused by lexical and stylistic phenomena (collocational differences, lexical ambiguity, translator's false friends or cognate lexemes, differences in conventionalised referencing, clich and idioms)~\cite[Table 5, p.22]{Kunilovskaya2022err}. In absolute counts, inability to recognise and reconcile cross-linguistic collocational difference is the leading source of disfluency in translations. 

To operationalise the specificity of student translations on the lexical level, we introduced two additional feature sets avoiding surface string features and building on the best practices in the field.  

The first feature set is designed to reflect collocational properties of translated texts. It includes proportions of n-grams with various degrees of semantic association between components to all types of linear combinations in a document, i.e. to all n-grams. 

Our approach to the design of collocational features is inspired by findings in the learner corpus research. In particular, a series of publications by Bestgen and Granger~\cite{Bestgen2014,Bestgen2017,Granger2017} focuses on the use of two association metrics, \gls{MI} and t-score, to study phraseological development in foreign language learners. They correlated the association scores with the learners' text quality. Their publications introduced the concept of \textit{collgrams} defined as n-grams with an association score above an arbitrary threshold~\cite{Bestgen2014}. The authors demonstrated that various measures generates from association scores had different correlation with the text quality and proficiency levels. The study confirmed their expectation that higher average MI score was a reliable sign of more advanced language skills, while t-score was less predictive of text quality. 
There is some evidence that negatively-collocated bigrams (anti-collocations), i.e. bigrams with the association score below zero, can be an even more reliable indicator than collgrams with a score above a reasonably high threshold, or than all collgrams with score above 0. 

A separate indicator in their study is the frequency of n-grams that were not assigned a association score, because some of the components were absent in the reference corpus. The relation of this feature to quality is disputable, because unseen tokens can be either errors or instances of creative writing. 

\label{pg:why_collocations}
Based on their findings, we expect that translations would have a higher ratio of t-score-based highly-associated bigrams, and lower ratio of MI-based highly-associated bigrams than in the comparable subcorpus of non-translations.
This hypothesis is based on the known properties of translation to prefer high-frequency, more established items, known as `lexical teddy-bears'~\cite[p.123]{Johansson2008}, and on the known properties of the association measures. MI is known to favour sequences made of low-frequency items, while t-score assigns higher scores to high-frequency items~\cite{Gries2010}. The observation that collocations of high-frequency words are less typical of native writers should be applicable not only to learner language, but to translations as well. 

The ratio of negatively associated bigrams and the ratio of bigrams not seen in the reference is aimed to capture less usual sequences which can be a sign of shining-though or errors, including acceptability in the register (e.g. \textcyrillic{\textit{ ,  ,  }}, mis-collocations in Russian similar to \textit{flowery tunnel} (instead of winding tunnel), \textit{to lose a train}).

We also included a collocational feature proposed by~\citet{Volansky2011}, i.e. average mean association score for all identified collocations, as well as standard deviation for the association scores to reflect homogeneity in the use of collocations. In our implementation, we averaged association scores for items with a score above zero, excluding anti-collocations from the counts to avoid negative and positive scores canceling each other. A higher average association score indicates more collocations. 
Based on the previous findings, we expect translations to be less collocated than non-translations, contrary to normalisation hypothesis.  

All collocational features were counted for bigrams and trigrams.

To sum up, the collocational features for each association measure and each size of n-gram include: 

\label{pg:collgrams}
\begin{enumerate}\compresslist{}
	\item ratio of highly-associated collgrams to all n-grams in the document (the cut-off for high association was set to recommended \gls{NPMI} > 0.5 and t-score > 6),
	\item ratio of negatively-associated collgrams~\cite[anti-collocations,][]{Evert2009} to all n-grams,
	\item ratio of all detected collgrams with the score > 0 to the total word count,
	\item ratio of bigrams absent in the model to all n-grams,
	\item average association score for all detected collgrams with the association score > 0,
	\item standard deviation for the association scores in each document.
\end{enumerate}

The ratios were computed based on n-gram types, not tokens. 

This gives us 24 features for each document in the corpus (six features by two scores by two n-gram sizes).

\paragraph{Extraction details} For each language, each association measure and each n-gram size, we trained a collocation detection model on large register-comparable corpus resources obtained for this purpose for each language and described in Section~\ref{ssec:ref}.

All lexical features were produced from lemmatised corpora. 
Further on, to reduce the impact of topical differences and of proper names, we removed all identifiers of personal, temporal or spacial deixis, i.e. words referring to a specific time, place, or person in context from both training corpus resources and research corpus.

In practical terms, all words tagged as proper names and their sequences were replaced with a placeholder \textit{PROPN} and all numbers were represented as XXX (e.g. \textcyrillic{\textit{{  }} -> PROPN}, 1984 -> XXXX, 10,527 -> XX,XXX, \textcyrillic{\textit{1980- -> XXXX-}}). 

Besides, we deleted all punctuation, except end-of-sentence marks.

The models were trained using the \textit{Phrases} module from the \textit{Gensim library}, version 4.2~\cite{Rehurek2010}, a free open-source Python library designed to process text for machine learning.
The two association measures were implemented as follows. 
We used the normalised variant of \gls{PMI}, suggested by~\citet{Bouma2009}, which imposed a fixed upper bound on the standard measure of association between components of an n-gram. Generally, PMI quantifies the ``mutual expectancy''~\cite[(Firth 1957, 181) as quoted in][]{Evert2009} between words. It is the measure of how strongly one word predicts the other, given their individual frequencies and the observed frequency of their combination. \cite{Evert2009} grouped PMI with effect size measures, which ask ``how much does observed co-occurrence frequency exceed expected frequency?''.

The standard \textit{Gensim} implementation used the NPMI formula shown in~\ref{eq:npmi}

\begin{equation}\label{eq:npmi}
\begin{split}
NPMI = \frac{ln(p(word_a, word_b) / (p(word_a)*p(word_b))}{ -ln(p(word_a, word_b))} \\
\text{where probability of word:}\\
p(word) = \frac{word\_count}{corpus\_word\_count}
\end{split}
\end{equation}

T-scorer was custom-made and computed the score based on the formula suggested by~\citet{Gries2010}. Unlike (N)PMI, t-score belongs to the category of significance tests. This score indicated  how unlikely was the null hypothesis that the words were independent.

T-score measure was calculated using the formula~\ref{eq:tscore_gries} after~\citet{Gries2010}. 

\begin{equation}\label{eq:tscore_gries}
\begin{split}
tscore = ln(\frac{bigram\_count - expected}{\sqrt[2]{expected}}) \\
\text{where:}\\
expected = \frac{word_a\_count * word_b\_count}{corpus\_word\_count}
\end{split}
\end{equation}

Both association measures are computed with minimum n-gram count of 10 using natural logarithm. % logarithm to the base $e$.
 
In both measures 0 means independence of n-gram components. NPMI lies in the [-1: 1] range, where -1 signals complete independence for words that never occur together, and +1 means perfect correlation. T-score bounds were experimentally established within [-11: 9] scope.
To train the models, we set the association score threshold to the lower bound of each metric.

% low thres  results in the noisy output that includes :: and ::, ::
While learning \textit{Phraser} models, we allowed for intervening words from the functional word classes, including adpositions, coordinating conjunctions, determiners, numerals and possessive pronouns. It gave access to items like \textcyrillic{\textit{::::}} (alpha::and::omega), \textcyrillic{\textit{::::}} (non::aggression::pact), \textcyrillic{\textit{::::}}.

Then, the models were applied to the documents in parallel and unseen non-transition subcorpora.

\subsection{\label{ssec:ngram}N-gram Language Modelling Features}

The second set of 10 lexical features exploits n-gram frequency lists and n-gram language models that were build from the same independent corpus resources as collocational features. These features reflect the distribution of n-grams in translated and non-translated texts, and replicate some of the QuEst++ blackbox features~\cite{Shah2013}. 

Word rank is a common translationese indicator which captures changes in the frequency profiles of lexical items in translated language. Translations are expected to use more of more frequent words, on the one hand, and more of \textit{hapax legomena} and very low frequency items or \gls{OOV} words, i.e. of `strange strings' unseen in large non-translated corpora, on the other hand. 

The implementations in the literature include measuring the size of the `list head', covering top 1\% of the frequency list from respective corpora, proposed by~\citet{Laviosa1998}, mean word rank used by~\citet{Volansky2015}, percentage of n-gram in each of frequency quartiles in a corpus of the source or target language (features 1046-1057) for sentence- and document-level \gls{QE}~\cite{Scarton2016}. \citet{Sominsky2019} extended this approach to cross-lingual setting and compared the number of words in each of the seven bins defined in pre-trained frequency lists using word alignments.

Another popular approach to capture translational specificity at lexical level is using the measures of confusion from non-translations \gls{LM} when it is presented with translated texts. \gls{LM} probabilities and perplexities are included in the pool of 17 baseline features in QuEst++ (features 1009-1014).
\cite{Nikolaev2020} used predictability of translations based on LM entropy measures to explore the impact of the typological distance between SL and TT on the properties of translations. 

\label{pg:ngrams}
This research relies on the following features:
\begin{itemize}\compresslist{}
	\item percentage of n-grams in the top frequency quartile defined on a large corpus of each language;
	\item percentage of n-grams in the bottom frequency quartile defined on a large corpus of each language;
	\item percentage of n-grams not seen in pre-trained frequency lists;
	\item LM perplexity averaged for document sentences;
	\item standard deviation for LM perplexities across sentences in the document. 
\end{itemize}

N-gram frequency lists (of orders 1, 2 and 3) from the training corpus, including calculation of frequency quartiles, were generated using QuEst++ feature extraction module, following the instructions in~\citet{Specia2016}.
These features capture the overuse of TL high-frequency items and, possibly, a higher ratio of OOV items. 
We used a trigram LM learnt on this corpus with \textit{KenLM} library~\cite{Heafield2011} to generate average sentence perplexities and their standard deviation for each document in our data. We hypothesise that this model should return higher perplexities for translated language. 

% https://stackoverflow.com/questions/43841467/how-to-compute-perplexity-using-kenlm

% import numpy as np
%import kenlm
%m = kenlm.Model('something.arpa')
%# Because the score is in log base 10, so:
%product_inv_prob = np.prod([math.pow(10.0, score) for score, _, _ in m.full_scores(s)])
%n = len(list(m.full_scores(s)))
%perplexity = math.pow(product_inv_prob, 1.0/n)

% Or using the log (base 10) prob directly:

%sum_inv_logprob = -1 * sum(score for score, _, _ in m.full_scores(s))
%n = len(list(m.full_scores(s)))
%perplexity = math.pow(10.0, sum_inv_logs / n)

% see also https://towardsdatascience.com/the-relationship-between-perplexity-and-entropy-in-nlp-f81888775ccc

N-gram features were generated from lemmatised and PoS-tagged version of the training and test corpora  (`lempos' format), where words were represented with their lemmas disambiguated with PoS tags (e.g. Are work suits on the way out? -> be\_AUX work\_NOUN suit\_NOUN on\_ADP the\_DET way\_NOUN out\_ADV ?\_PUNCT).

\section{\label{sec:sum2}Summary}
%\addcontentsline{toc}{section}{Summary}
This work is related to the studies showing that translations stand apart in the TL and tend to share a set of lexical, syntactic and/~or textual features that can be cumulatively called translationese. We bring together findings from corpus-based translation studies and translation pedagogy, contrastive linguistics, register analysis and learner language research to suggest a set of translationese indicators that would effectively distinguish translations from non-translations and might be predictive of human translation quality and professionalism. 

We defined translationese as a set of distinctive statistical characteristics of translations that distinguish them from non-translations in the same register and make them a language variety. Although the professional norm in some registers might demand more covert translation than in others, there is ample evidence that translations are always distinguishable from non-translations regardless of any external parameters, including of level of competence. Translationese is viewed as a by-product of the translation process, and it constitutes an unalienable property of all translations, the property of being a translation.  

A high-level theoretical explanation for the specificity of translations, translation universals hypotheses, was faced with lack of empirical support, but yielded a consolidated understanding that translations bare traces of the source language as one of their strong predictors. There are also persistent characteristics that make translations their own type of language production distinct from non-translations, such as longer sentences, more connectives and clauses per sentence, lower TTR and other patterns that are neither TL- nor SL-oriented.

The link between the amount of translationese manifested in a document and its translation quality has been often assumed or implied in translationese studies, as well as directly explored in corpus-based setups. Although translationese is related only to the fluency aspect of quality, there are good reasons to believe that, in human assessment, quality judgments are highly influenced by the overall acceptability and readability of a translation, i.e. by how well it blends in with the expected TL norm. 
It is reasonable to assume that the easier a text is recognised as a translation, the lower the quality, especially for some registers, if not for others. 

The final part of the chapter presents three sets of translationese indicators selected and implemented taking into account the research goal and previous observations in a number of related research fields.  

The outcomes of the experiments reported in this thesis largely depend on whether useful features were included in the document representation in the first place. 
First and foremost, the features were selected to capture the specificity in the linguistic makeup of out-of-English Russian translations anticipated in practical translation textbooks, and indicative of translation quality and level of professional expertise. Many of them reflect the contrastive differences between English and Russian. Feature selection was also guided by such principles as cross-linguistic comparability, reliability of extraction, frequency, interpretability and ability to abstract away from the content.

We proposed an extended set of morphosyntactic features extracted from UD annotation and two smaller sets of abstract lexical features that capture collocational and distributional properties of translations. 

%In our experiments we will use TF-IDF for sanity check, and vectors from a neural language model to show how competitive our approach is against state-of-the-art NLP approaches to language representation.

\chapter{\label{cha:fin}Conclusion}
%\todo[inline]{An suggests that chapters should start on odd page}
This thesis applies translationese theory to the study of human translation quality. 
Using machine learning methods, it explores the extent to which translationese properties of text can explain the quality judgments obtained by holistic assessment of translated documents, error annotation and direct assessment of sentences in context. Additionally, we use professional and student translations as ontological varieties that can be related to quality. 

The investigation has an explanatory focus. It employs linguistically-motivated features to discern patterns in translational behaviour and individual types of translation solutions that can be associated with lack of professionalism or lower translation quality. Our findings contribute to translationese studies and are applicable in translation education. 
Besides, the thesis offers a comprehensive comparative study of translation quality assessment methods, including those used to benchmark translation quality in machine translation. It sheds light on the important properties of quality labels and scores originating from various approaches to manual quality assessment. 

To prove the hypothesis that translationese properties are related to translation quality, we need to show that the features useful in predicting translationese can also be used to predict translation quality and/or that translationese-related phenomena are more seen in lower quality translations. For example, \citet[][p.357]{Hu2021} conclude their multilingual translationese study by stating: ``translations tend to have more relative clauses. This seems to be truly universal despite obvious differences in the source languages''. If they are right, normalised frequency of relative clauses in a translated document should be among strong translation predictors for English-Russian language pair. Further on, if our hypothesis stands, relative clauses should be on the list of strong quality predictors and/or by relatively well correlated with the quality label or score. 

The relation between translationese and quality has been suggested in the literature many times (see Section~\ref{sec:feats4qua}). However, we are not aware of research that would put this proposition to an empirical test by machine learning methods, at least, for human translations. This thesis frames the problem as a human translation quality estimation task supported by statistical testing and feature analysis. Our major focus is on document-level quality, which is known to be under-researched in MTQE, including due to lack of annotated data and limitations of representation learning. Importantly, out hypothesis makes us rely on predicting quality using the target language side of the parallel corpora, while ignoring the relation between ST and TT. We contend that in human translations produced in good faith, deviations from semantic similarity between ST and TT would usually distort the natural flow of text and create weird nonsensical renditions suggestive of accuracy errors. 

The aim and the methodology of this research place it at the interface of a number of disciplines. It builds on the knowledge and findings in 
\begin{itemize}\compresslist{}
	\item translation studies, especially in the areas of translationese studies, \gls{TQA}, professional competence research and pedagogy;
	\item corpus linguistics, especially building learner and parallel corpora and corpus analysis methods; 
	\item \gls{MTQE}, and applications to HTQE;
	\item \gls{ML} and \gls{NLP} for computational methodology.
\end{itemize}

\paragraph{Contributions}
%\todo[inline]{An suggests that there should be more cross-referencing in contributions, like page ranges}
Our contributions can be summarised as follows: % they span several research areas
\begin{enumerate}\compresslist{}
	\item \textit{We designed and rigorously tested three sets of linguistically-motivated translationese indicators for English-Russian translations of mass-media texts}. 
	
	They include 60 morphosyntactic features extracted from Universal Dependencies annotation and two smaller sets of abstract lexical features, including 24 collocational features and 11 features from n-gram language models. The selection process was guided by translationese theory as well as by findings from previous empirical studies. In particular, these features include most translationese indicators from a comprehensive set developed by~\citet{Volansky2015}, which is used as a baseline in many translationese studies. With regard to the UD features, care was taken to include only cross-linguistically comparable isomorphic features informed by the contrastive analysis of English and Russian that could hypothetically pick specificity of language choices in the situation of translation (Section~\ref{sec:myfeats}). 
	
	Our experimental results on two independent collections of translations proved that structural morphosyntactic features were by far more effective in distinguishing translations and non-translations. They were also very competitive against vector representations from pre-trained contextualised word embedding models, including a dedicated Russian language model and a multilingual \textit{mdeberta3}, the current SOTA in generic representation learning (Section~\ref{sec:detect}).      
	
	\item \textit{We identify most prominent translationese indicators for English-Russian mass-media translation, associated with `shining-through' trend in translational behaviour, using bottom-up approach}. 
	
	Multivariate analysis through feature selection revealed a large intersection between sets of translationese features that were most effective in predicting student or professional translations against non-translations. In univariate analysis, these translation varieties were shown to exhibit similar trends in translational behaviour along the same dimensions of analysis, most notably, `shining-through'.
	
	The most prominent translationese indicators were among syntactic, rather than morphological, features, and signalled a tendency to produce longer and more complex sentences in translation. We confirmed some of the translational distortions anticipated by translation theory and practical textbooks on English-to-Russian translation. Namely, frequencies of the following items tend to be higher in translations than in non-translations: additive discourse markers, analytical passives, copula verbs, modal predicates, personal pronouns, finite verbs and determiners.
	The explorations at the lexical level indicated that translations were likely to have more of unusual strings, unseen in a large TL model. They relied on a smaller variety of collocations, and the collocation in translations were less familiar to the language model (Section~\ref{sec:bestof}). % had lower association score 
	
	\item \textit{We obtained evidence that professionalism-related differences between translations were much less related to translationese than distinctions between \textit{good} and \textit{bad} translations.} %However, a clear distinction among them can be to an extent interpreted as related to quality.
	
	Translationese-aware UD feature set was outperformed on this task by QuEst++ features. Despite QuEst++ success can be largely explained by differences in the STs, findings on prominent fluency and adequacy features demonstrated that student translations were more perplexing to a LM trained on non-translations, most likely due to `strange strings' and awkward word choice. Student translations were also found more repetitive. Analysis of a few strong translationese indicators, selected as predictive of professionalism, returned contradictory results. Some expected translational behaviours were actually more noticeable in professional translations, while other -- in student translations. For example, professional translations were found to contain longer sentences, more epistemic markers and modal predicates than student translations. However, on other translationese features, professionals demonstrated more ability to recognise and counteract translationese (more indirect objects and negative particles, fewer analytical passives).
	Besides, the association between translationese features and labels was very low, with many strong translationese predictors being irrelevant for professionalism opposition (Section~\ref{sec:labels}).
	
	\item \textit{We provided evidence that prominent translationese indicators were indeed predictive of document-level quality labels from holistic assessment confirming that the amount of translationese was well-associated with human perception of translation quality.}   
	
	Linguistically-motivated morphosyntactic translationese features were competitive against other representations on document-level quality labels and scores. They outperformed QuEst++ and were second only large pre-trained word embedding models. Feature analysis provided further overwhelming support for our hypothesis that translationese indicators can be useful in predicting quality. Note that this finding is limited to holistic assessment of document-level quality. We demonstrated that, in the experiment on binary quality labels, lower-ranking translations were always more distant from the expected target language norm and were well-associated with stronger translationese trends (Section~\ref{ssec:bin})
	
	\item \textit{We demonstrated that quality judgments from three assessment methods revealed dissimilar properties: they accounted for translationese to various extent and performed better at document or sentence level.} 
	
	Quality scores derived from error annotation and sentence-level direct assessment scores were shown to be less translationese-aware than document-level holistic perception. The study of association between translationese indicators and error-based scores returned ambiguous results. Some results were coherent with translationese approach to quality (the higher the score, the less translationese), while for some features higher-scoring translations manifested more translationese.
	Comparative performance of various representation demonstrated that error-based scores returned higher results at document level, while scores from direct assessment performed better at sentence level. % (i) dedicated sentence-level QuEst++ feature set and vectorised representations did not benefit from the shift from document to sentence level. (ii) the drop in performance caused by limiting the data to a (random) forth of the original size was bigger for documents than for sentences. (iii) DA returned higher results than any error-based scores on same-size sentence-level dataset (iv) Sentence-level results for DA scores were better than at document-level on all representations, but especially for QuEst++ (cf. da\_mean: $r=0.33$ vs $r=0.18$) and \textit{ruRoberta} ($r=0.39$ vs $r=0.22$). Curiously, this was not the case with error-based scores where we discussed a unanimous downward slide on all representations (see page~\pageref{pg:downward_slide}). 
	Additionally, in our setting there was no evidence that any attempted document-level representations returned better results for accuracy or fluency. Given that some of our representations did not have direct or indirect access to the SL, this outcome raises doubts about the practical reliability of this distinction at least at the document level (Sections~\ref{ssec:bin} and~\ref{sec:_scores}). 
	
	\item \textit{We release relatively large document- and sentence-level aligned datasets based on student translations with holistic document-level labels, error-based and DA scores that can be used for \gls{HTQE}}.
	% RusLTC-HTQE-binary rusltc-htqe-binary rusltc-htqe-scores
	The datasets contain English-to-Russian translations of mass-media texts from the \textit{RusLTC} and were produced by senior students majoring in translation at BA or MA (specialist) levels. 
	Documents with binary labels (360 documents) include several top and several bottom translations to 57 English texts that were offered to students as test assignments or translation competition tasks. Submitted translations were sorted by the final assessment result (agreed between jurors/examiners) that was returned to the student or announced at a competition.
	Error-based scores are frequencies of translation errors normalised to the document word count. The annotations were produced as part of translation education process across a number of years for multiple student translations to 46 English sources. This dataset counts 553 documents/12,369 sentences.
	Direct assessment experiment involved final-year BA students on a translation or linguistics degree programme. The dataset includes two most internally consistent ratings for 140 documents (3,224 sentences). This subset is in intersection with binary labels dataset and with error-annotated dataset.
	Detailed description of the datasets are presented in Section~\ref{sec:mygold}.
	
\end{enumerate}
%\todo[inline]{An suggests shorter summaries}
\paragraph{Summary of the chapters}
This paragraph offers a brief overview of the essential component of this thesis presented in chapters.
Each of Chapters~\ref{cha:indicators}-~\ref{cha:quest} is a unity of theoretical and practical part, which describes a separate stage in the project. These chapters open with conceptual foundations and empirical evidence from previous work to explain practical implementations described in the last section of each chapter. 

\textbf{Chapter~\ref{cha:indicators}} introduces the theoretical underpinning of this research rooted in translationese studies. It explains the approach pursued in this work in the context of previous and recent developments in translationese theory.
Section~\ref{sec:feats4qua} explains the hypothesised link between translationese and translation quality. The final section of Chapter~\ref{cha:indicators} offers a detailed description of the proposed feature sets, including their selection and extraction, motivated by the research goal, our stance on translationese and adopted methodology.

We defined translationese as any statistical deviations of translations from comparable non-translations, regardless of their origin and nature. Following the overwhelming evidence from previous research, presented in Section~\ref{ssec:keyterms}, our approach assumes that shining-through and normalisation are major translationese trends. 

The overview of most prominent methodologies in translationese studies demonstrates how theoretical foundations and research goals are linked to feature engineering. We characterise corpus-based and computational methodologies that can rely on surface, linguistic or linguistically-motivated features, depending on the amount of external resources and knowledge is necessary to extract the features. The choice of theoretical approach and major research designs in translationese studies are presented as following top-down or bottom-up approaches. Researchers can pursue linguistic-theoretical and explanatory goals or be after most effective engineering solutions for a related NLP task. 
Along these dimensions, this research can be described as an explanatory bottom-up approach based on computational methods and linguistically-motivated features. 

Section~\ref{sec:feats4qua} brings together theoretical and empirical evidence that translationese can be predictive of translation quality and levels of professional expertise. Most studies that put this hypothesis to a test were corpus-based and traced a few individual hand-picked features (like sentence length or adverbial quantifiers). There were a few computational studies that explored this relation in the context of MT. In this section, we discussed the feasibility of a quality estimation approach to human translation that relies only on the properties of translations and effectively ignores accuracy as an aspect of quality. This approach is assumed in a translationese-based HTQE. Briefly the major pro-arguments include reported evidence for the lack of distinction between accuracy and fluency in annotation outcomes, greater subjective impact of disfluencies in translation perception, including leading to comprehension difficulties, findings from comparative analysis of professional and student translations and error analysis. 

Finally, Section~\ref{sec:myfeats} describes three feature sets proposed in this thesis. We hypothesised that they should be effective in predicting translations and translation quality. We proposed a set of 60 structural morphosyntactic features that included normalised frequencies of default syntactic relations, morphological forms and categories as annotated in \gls{UD} framework as well as features that relied on more sophisticated extraction algorithms such as mean hierarchical distance or cumulative frequencies of listed causative discourse markers, for example. 
The key principles that guided the selection of structural features included the following considerations. They were contend independent, cross-linguistically comparable (hence, language pair specific), motivated by observations from translation practice textbooks, corpus-based error analysis, previous research in translationese studies. We also heeded interpretability and reliability of extraction.
In an attempt to pick translationese phenomena that can be manifested at lexical level we proposed two sets of abstract lexical features that were not based on surface strings directly. They reflected collocational and distributional properties of translations. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Chapter~\ref{cha:varieties} discusses previous work related to learner translator corpora, the study of professionalism in translation and building register-comparable corpora for translationese studies. Its major aim is to present the textual data used in this research and to provide motivations for corpus building solutions.

Our discussion is focused on issues of using student data annotated in real-life contexts as research data and on findings about differences between professional and student translations. Student and professional translations reveal a lot of similarities between them, and researcher usually find less differences than expected. 
There were attempts to interpret differences between students and professional from the point of view of translationese theory, including relating these distinctions to quality. 
 
Importantly, Section~\ref{ssec:norm} discuses the issue of datasets and resources comparability in translationese studies. We explore the concept of comparability in NLP and in corpus-based research. The section shows that the more important aspect of comparability in translationese studies in related to register to use a term which reflects text-internal approach to functional text categorisation. Functional text comparability of parallel corpora and reference corpus is key in translationese studies, because registers are known to trigger different types of translationese and  professional translational norms are register-specific. Building register-comparable corpora is treated as an important preliminary step in this research. 

Section~\ref{ssec:subsets} has the description of data collection. As student translations are our major type of text and they come from our own corpus project, we shared our experience with an emphasis on technological issues in learner translator corpus-building. In our experience they are associated with handling multi-parallel data and adding error annotation to a parallel corpus. The section presents the rationale behind selecting various subsets of student translations from the \gls{RusLTC} aimed at obtaining homogeneous collections for each experiment. 
In this section we refer to previous research on \textit{Functional Text Dimensions} framework that was used increase comparability of subcorpora in this research. 
These efforts included processing a English sources from professional and student parallel corpora in a monolingual setup: we calculated the most comparable subsets of professional and student parallel corpora, and a cross-lingual task aimed at finding a subset of Russian non-translations most similar to the joint collection of English sources. 
Finally, this thesis relies on large language collections of mass-media texts in Englshand Russian, used for training LMs. These resources are also introduced in Section~\ref{ssec:subsets}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Chapter~\ref{cha:quest} introduces key theoretical concepts related to translation quality, and discusses existing practical approaches to manual assessment and automatic quality estimation. Importantly, it contains the description of annotation experiments and reliability studies associated with the quality judgments used as learning targets in this research. 

Throughout the chapter we maintained a comparative focus bringing together approaches in TS and in MT with regard to quality. We demonstrated that TS typically describes quality in terms of three aspects, with adequacy having priority over accuracy and fluency. MT researches express doubt that these distinctions are practical from the point of view of reliable annotation, and if quality is disaggregated into aspects, accuracy and fluency are distinguished. 

Another point of divergence between HT and MT is the level of quality judgments. In TS word-level of sentence-level quality are difficult notions, while most research in MT quality estimation/evaluation is done on these levels, the typical use cases of MT. Research at document-level is also held back by lack of annotated resources and limitations of existing approaches in representation learning. 

Section~\ref{sec:qe} describes translation quality estimation as a computational linguistics and NLP task in MT, including best-performing approaches in MTQE and a few existing cross-overs to HTQE. Understandably, the potential of feature-learning approaches and neural networks is being actively explored in the field, but explainability remains a bottleneck. 

Section~\ref{sec:ass} has a detailed description of best practices in quality assessment: focused on grading in HT and on producing quality benchmarks for ML experiments in MT. It covers such competing methods as rubrics and error annotation (in HT), and error annotation, direct assessment, post-editing  (in MT). We notice that there is more pessimism in MT with regard to reliability of assessment, especially at document level, which is not upheld in our research. Also, MT has more awareness that various assessment methods capture dissimilar properties of translations, an property of quality assessment that was clearly revealed in this thesis. 

Finally, Section~\ref{sec:mygold} explained how the quality labels and scores used in this thesis were obtained and demonstrated their reliability. Our labels were based on the status of translators with regard to professionalism (students vs professionals) and on the outcomes of document-level holistic assessment, which was limited to contrasting categories of `good' and `bad' translations of the same ST. 
Quality scores were obtained from error-annotation and sentence-level direct assessment. 
The agreement between university translation teachers and final-year under-graduate students (DA experiment) ranged between 0.467 and 0.734 in various settings, with the lowest result seen for sentence-level DA by students.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Chapter~\ref{cha:translationese} presents the results of translation detection tasks based on professional and student translations. We refer to these experiments as translationese classifications, in which we test the ability of the proposed feature sets to distinguish translations and non-translations (i.e. register-comparable documents originally authored in the TL; also known to as reference corpus in translationese studies). A considerable part of this chapter is given to feature analysis and description of translational properties of classification categories, facilitated by the use of interpretable explicit features. 

This section provides details of experimental setup used in four classification tasks reported in this thesis. Our major learning algorithm was SVM with linear kernel. For sanity check we also implemented a neural classifier which returned very similar results. All classification tasks use a similar set of representations alternative to the proposed feature sets. They put the proposed features in the perspective of the achievable in each task. Besides, their comparative analysis of their performance facilitated the interpretations of various properties of documents in each category. Alternative representations were selected to offer a spectrum of representation approaches ranging from \textit{tf-idf} to multilingual \textit{mdeberts3}, the current SOTA in word-level representation learning. We also included cross-lingual sentence embedding models trained on \gls{STS} task and on DA scores in a MTQE task. While all documents in our classifications are in Russian, we added a large dedicated Russian word-embedding model.

Translationese classifications on both student and professional translations return high results in all settings. It is not surprising as most representations are content-dependent and capture topical differences between translations and non-translations. It is fairly easy to predict translation using strings of proper names only. Delexicalised theoretically-motivated morphosyntactic features proved to be very competitive against those very strong alternatives, while abstract lexical features, capturing collocational and distributional properties of documents did not live up to our expectations. 

The comparison of outcomes for student and professional translations allowed to reveal strong translationese indicators and establish specificity of each translation variety. Professional translations were characterised as being more homogeneous and predictable, demonstrating fairly stable language use than student translations. Despite our efforts to obtain functionally comparable collections, we established that English texts translated by professionals might have been structurally more complex, which was a confounding factor in our experiments.

Feature analysis was based on \gls{RFECV}, a feature selection algorithm that determined a best-performing subset of most contributing features. It helped to identify strong translationese features, which were additionally verified by statistical analysis. We have seen that features included in multivariate patterns by feature selection would often have reasonably high correlation with the labels in a single-feature classifier setting. 

Establishing good translationese predictors does not allow to describe properties of translations by itself. To understand actual trends in translational behaviour and to prove or disprove the theoretical expectations that guided feature engineering, we performed three-way comparative analysis, looking at frequencies of features in sources, targets and translations. 
Depending on the outcomes of these comparisons we were able to characterise features as contributing to \textit{anglicisation, shining-through, overuse of ST items, underuse of TL items, normalisation, adaptations}. They cover a continuum of translational strategies defined by the amount of active attempts aimed at reconciling the differences between SL and TL.

Our research confirmed that carrying over SL frequency patterns into translations, whenever possible, was the major factor explaining translational deviations from the expected TL norm. Recall that we tried to keep our features cross-linguistically comparable and more or less open too alternative solutions, and not rigidly imposed by the language system. There were more translationese indicators associated with this translational behaviour than with the solutions which involved selecting restructuring original sentences and using non-isomorphic Russian items. 
SL/TL-independent translationese was more often seen in student translations. For example, the used more additive discourse markers than were seen in their STs or than was expected in the TL. 
One overwhelmingly strong trend seen on several syntactical features that cut across both translation collection was sentence lengthening and increased syntactic complexity.

Although abstract lexical features were not as successful as UD features in the translation detection task, their analysis revealed some additional properties of translations. For example, student translations were found more perplexing to an n-gram language model, which might suggest reduced fluency. 

%%%%%%%%%%%%%%%%%%%%%%%%%%
Chapter~\ref{cha:pro_qua} is the main experimental chapter, which brings together classification experiments on professionalism and quality labels, and regression experiments on document- and sentence-level quality scores. 

In this chapter the performance of translationese indicators is contrasted with the performance of QuEst++ features, a well-known set for MTQE. 

Although professional and student translations were well-distinguishable on translationese-aware features, the major differences between them were not associated with translationese. Feature analysis demonstrated that professional translators might be even more prone to translationese than students along some dimensions. Higher predictability of professional translations can be partly explained by their higher homogeneity arising from more stable behaviour than that in student translations.
However, our results are less reliable than expected, because professional and student parallel corpora demonstrated some differences in the STs that were interfering with the interpretation. 

The experiments with binary quality labels, which reflected document-level assessment based on holistic perception of student translations, yielded support for our hypothesis that lower-ranking translations should exhibit more translationese than higher-ranking translations. UD features achieved comparatively good results in this task, returning F1 of 0.68 on the best-performing feature subset. This result was secondary only to \textit{mdeberta3} vectors with $F1=0.78$. Feature analysis demonstrated that strong translationese indicators were indeed among the strongest quality predictors, and that lower-ranking translations were more distant from the expected TL norm than higher-ranking translations. 

The experiments on quality scores were less convincing in terms of performance, but they were enlightening about the properties of error-annotation and sentence-level Direct Assessment as methods of generating quality scores. 

Comparative analysis of the regression results on a number of representations and feature analysis indicated that error annotation reflected translationese properties of documents to a greater extent than DA scores. But this is probably because error-based scores are essentially document-level metrics. Recall that translationese is also by nature a statistical property, which is more visible at document level. The performance of error-based scores at sentence level was twice lower than at document level, which was not the case for DA scores. 
Feature analysis results for error-based scores were at best ambiguous: for some features they were in line with translationese hypothesis of quality, but for others they contradicted the main assumption. 

Feature analysis for DA assessment scores had a tendency to contradict the expectation that more translationese means lower quality. Many well-expressed translationese features positively correlated with DA scores. It means that this type of assessment was ignorant of translationese phenomena. Probably, it is not surprising, as DA scores returned better results than error-based score at sentence-level and were shown to be much better correlated with accuracy score than with fluency scores.

In our experiments, there was \textbf{no evidence that either accuracy or fluency scores were better aligned with the properties of texts} as represented by seven various methods, including those that were not designed to pick semantic inaccuracies due to no access to the SL side of the corpus. However, at sentence level the difference in performance on accuracy and fluency was seen for UD, QuEst++ and \textit{mdeberta3} even for a small dataset of 3,224 random sentences. % results are different on cross-annotated! 
This invites a conclusion that error-based judgments capture local issues in translations and should not be used to generate overall quality scores, especially across all sentences in a document. At the same time, in our setup we cannot rule out the possibility that accuracy errors do introduce disfluencies in human translation, and therefore, scores based on content errors can be learnt using representations that do not have access to ST (such as \textit{ruRoberta}). Essentially, this line of interpretation negates the difference between aspects of quality for any experiments that are not aimed at word-level quality. 

With regard to the \textbf{differences between results for error-based and DA scores}, they were lower for any error-based score than for DA (da\_mean) on all representations, given a fair random subset of 3,224 sentences (cf. lower part of Table~\ref{tab:sent_err_double} and Table~\ref{tab:da_sent_res}). We interpreted this difference as reflecting the particularities of the annotation setup. Unlike error annotation, raters involved in DA were asked to pass judgment on every sentence. In error-annotated corpus 58\% of sentences had at least one span annotated. This also explains lower correlation between error-based and DA scores at sentence level than at document level. For example, for document-level unweighted accuracy and mean DA score: $r=0.509$ and for fluency: $r=0.169$, while at sentence level those correlations were $0.291$ and $0.149$, respectively.
The correlation between quality scores from two manual assessment methods indicates that in direct assessment setup humans tend to give preference to accuracy over fluency. %, at least when the scores are aggregated at document level. 
Moreover, a suggested weighting scheme based on TAUS recommendations, improved the correlation between error-annotated and perceived quality. It justifies the perceived difference between various error categories and between severity levels. 
% Same UD and especially QuEst++ features contributed to all scores while others remain specific to accuracy or fluency.

Concerning the performance of translationese indicators on various types of quality judgments, we can conclude that each of them captured a specific aspect of quality, which may or may not be related to translationese. Holistic document-level perception manifested in binary quality labels was shown to be the most translationese-aware quality judgment.
Lower-ranking (\textit{bad}) translations clearly demonstrated expected translationese tendencies (see page~\pageref{pg:bad_tendencies}).
Findings from feature analysis in experiments with error-based document-scores indicated that this type of judgment might be focused on other aspects of translation quality, which did not reflect translationese properties.
Note that these conclusions were only possible as a result of feature analysis on hand-crafted feature sets. The overall classification results were not enlightening as to which properties of translations were picked by the classifiers and regressors. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Limitations and Future work}
This research has a number of limitations. They can be arranged around the major steps of this research.
Some limitations are suggestive of future work.
 
The most obvious and painful limitations are have to do with feature design and extraction. We limited ourselves only to those theoretically known translationese phenomena that we were able to extract with relative reliability. 
However, the proposed features can be ambiguous in terms of interpretation.
For example, the use of any types of pronouns, especially possessive and indefinite/total is expected to be different under the pressure of the ST. Although our analysis failed to find empiric proof for that, we will be weary of dismissing pronouns as a translationese give-away. It is likely that this feature requires taking into account some context which was ignored in our approach. A more effective feature set might take into account specific structures which are expected to be translationese-prone.

The proposed approach to feature extraction does not link ST and translation items. Although we rely on document- and sentence-level aligned corpora, there is no reliable way to say that specific ST items triggered the overuse of isomorphic structures in translations. It can be argued that this connection is useful only for detailed descriptions of translators' solutions, and not for overall characteristics of translated language. The same challenging English items can push up frequencies of various Russian items depending on which coping strategy is preferred by a translator. For example, English infinitives functioning as adverbial modifiers of purpose (structures like \textit{to do X, Y did Z}) do not have a parallel structure in Russian. Typical solutions involve (i) a infinitive clauses necessarily introduced by a marker of inferential relations (e.g. \textcyrillic{\textit{чтобы противостоять этой реакции, Кремль мобилизовал как вещательные, так и онлайн-СМИ}}), (ii) a prepositional phrase with a deverbal noun (e.g.  \textcyrillic{\textit{для формирования общественного мнения в интернете Кремль создал мощную цифровую инфраструктуру из ботов и платных авторов}}) or (iii) a predicative complement construction with a verb of purpose (e.g. \textcyrillic{\textit{Кремль пытается противостоять этой реакции путем мобилизации троллей}}). These solutions have various degrees of similarity to the original structure (with the first one being most literal) and drive up counts for various translationese features. 

Nonetheless, developing more context-aware bilingual features might be more beneficial for interpretation.
An attempt to contextualise translationese features was made in~\cite{Eetemadi2015} and developed by~\citet{Sominsky2019}. They proposed leveraging word-level alignments to produce a vocabulary of lexically anchored minimal translation units inspired by phrase-based machine translation techniques. A simpler approach based on human translations is described in~\citet{Hu2018}, who used constituent parse trees and dependency triples as translationese features.

%Interpretability of the results is limited due to lack of item-level alignment. We are not sure that a reliable automatic solution is possible for this problem. Our experience in manual annotation of ST items that might have triggered translation errors (see~\cite{Kunilovskaya2022err}, shows that this task was challenging even for professional humans due to considerable re-working of the sentence structure, required by dissimilarities of the SL and TL systems. These transformations often affect several language levels, too.

It is well-known that one of the major syntactic problem of human English-to-Russian translations is word order. However, our features do not focus specific translationese-prone structures, such as time and place adverbials and prepositional phrases in the absolute sentence end. Frequencies of bigrams, even PoS bigrams are not specific enough to capture these phenomena.
%I should have included features that capture beginning and end of sentences (I never thought about him) 

Our feature analysis was by design limited to capture translationese associated with SL/TL contrast.
However, deviant frequencies of items in translations can be generated by other factors as well. For example, it is unlikely that increase sentence length and sentence complexity can be entirely explained by the languages involved. We agree that this trend might be related to coping with cognitive load and lack of paraphrasing skills and flexibility, i.e. by deficiency in productive aspects of TL competence.
% lack of evidence for Not selected by RFECV for either pro-ref or stu-ref (13): obl, appos, advers, cconj, discourse, epist, possp, tempseq, flat, anysome, compound, interrog, conj

With regard to the data collection, we have shown that the outcomes of translationese classifiers on student and professional translations, and experiments on professionalism were affected by the differences in the underlying source text collections. This limitation was to an extent remedied by feature analysis, but calls for a special treatment of translations based on diverging ST collections. For example, \citet{Popovic2020} proposed to minimise the effects of diverging ST collections by using \textit{relative difference} between ST and TT as values for TT features.

In terms of experimental setup, we envision several possible extensions to further explore the relation between HTQE and translationese properties of translations. They include ideas that had to be left outside the scopes of this research due to considerations of space. 

A straightforward approach to test the relevance of translationese properties to quality prediction is proposed  by~\citet{Aharoni2015}. They used translationese classifier probabilities for translated class as a single input to a quality predictor on the assumption that the higher the translationese classification accuracy, the lower the expected translation quality.
In future we want to try merging UD features and embeddings to try and improve the results on quality labels. Current results make us hypothesise that manual features and embeddings capture different quality-related aspects of translated documents.

An important insight from our analysis is that human perception, at least at sentence level, emphasise accuracy. It is obvious that a well-rounded effective approach to automatic quality estimation should te able to measure similarity between ST and TT. We hope to improve our previous attempts to complement translationese approach with ST-TT semantic similarity module, and develop a effective NLP solution for document-level human translation quality estimation. 

Finally, our error-annotated data is big enough -- it is the size of the WMT datasets, at least -- to test neural solutions for word-level quality estimation task existing in MT and to adapt them to predict error spans in human translations. This direction of future work might be forestalled by the challenges of converting \textit{brat} error annotations into a reliable ML dataset.    

%LongFormers needed: sentence-level analysis does not give an adequate explanation of the choice of syntactic structures, which often reflect the flow of information in a coherent discourse unit.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In sum, holistic human assessment was reasonably well-aligned with the amount of translationese, especially in its shining-through form, while error-based and DA-annotated quality captured other properties of translations that were unrelated to manifestations of translationese. The distinctions between professional varieties were not rooted in translationese, either. 

We noticed that error-based scores for accuracy and fluency did not reveal the expected difference in performance on representations that, by design, were able to capture the TL properties only.
This confirmed our intuition that in human translation, \textbf{inaccurate translations are usually also disfluent}. Hence, a reliable approach to capture fluency can be a useful proxy for overall quality in human translation quality estimation.

%%%%%%%% THE END %%%%%%%%%

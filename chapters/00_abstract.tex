\addcontentsline{toc}{chapter}{Abstract}

% Experiments with document-level quality labels confirmed that lower-ranking translations bear signs of passive translational behaviour associated with transfer of ST patterns and failure to introduce items, specific to the TL.

\chapter*{Abstract}
% motivation and gap
Human translation quality estimation is a relatively new and challenging area of research, because human translation quality is notoriously more subtle and subjective than machine translation. At the same time, human translation is routinely assessed by education and certification institutions, as well as at translation competitions. Do the quality labels and scores generated from real-life quality judgments align well with objective properties of translations? This thesis puts this question to a test using machine learning methods.

% hypothesis and rationale
% Goal: Compare automatic prediction results (learnability) for several quality labels/scores on a number of representations
Conceptually, this research is built around a hypothesis that the amount of translationese manifested in a translation can be predictive of its quality. This assumption is often made in translation studies but has never been put to a rigorous empirical test. Exploiting translationese features for quality prediction can help identify quality-related trends in translational behaviour and provide data-driven insights into professionalism to improve training. Translationese is a document level property, which fits well with the concept of quality in translation studies. Translationese features are also more interpretable and can explain linguistic differences between human translation quality categories.

% features/reps
We investigated an extended set of Universal Dependencies-based morphosyntactic features, inspired by previous research in translationese, as well as two lexical feature sets capturing (i) collocational properties of translations, and (ii) ratios of vocabulary items in various frequency bands along with entropy scores from n-gram models. To test our features against a challenging competition in translationese classifications and in quality estimation tasks, the experiments were run on \textit{tf-idf} features, \textit{QuEst++} features and contextualised embeddings from a range of pre-trained language models, including the state-of-the-art multilingual solution for machine translation quality estimation. Our major focus was on document-level prediction, however, where the labels and features allowed, the experiments were extended to the sentence level.

% design: data, labels, methods
The corpus used in this research includes English-to-Russian parallel subcorpora of student and professional translations of mass-media texts, and a register-comparable corpus of non-translations in the target language. Quality labels for various subsets of student translations come from a number of real-life settings: translation competitions, graded student translations, error annotations and direct assessment. We overview approaches to benchmarking quality in translation and provide a detailed description of our own annotation experiments. 

% results and interpretation
Of the three proposed translationese feature sets, morphosyntactic features, returned the best results on all tasks. In many settings they were secondary only to contextualised embeddings. At the same time, performance on various representations was contingent on the type of quality captured by quality labels/score.
Using the outcomes of machine learning experiments and feature analysis, we established that translationese properties of translations were not equality reflected by various labels and scores. For example, professionalism was much less related to translationese than expected. 
Labels from document-level holistic assessment demonstrated maximum support for our hypothesis: lower-ranking translations clearly exhibited more translationese. They bore more traces of passive translational behaviours associated with following source language patterns whenever possible, which led to the inflated frequencies of analytical passives, modal predicates, verbal forms, especially copula verbs and verbs in the finite form. As expected, they were more repetitive and had longer, more complex sentences. Higher-ranking translations are indicative of greater skill in recognising and counteracting translationese tendencies. For this quality type, translationese indicators might provide a valuable contribution to an effective quality estimation pipeline. 

However, error-based scores, and especially scores from sentence-level direct assessment, proved to be much less motivated by translationese and fluency issues, in general. This is confirmed by relatively low regression results across all representations that had access only to the target language side of the dataset, by feature analysis and correlation between error-based scores and scores from direct assessment.
